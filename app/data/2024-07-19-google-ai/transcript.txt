 Google, Google, Google y la inteligencia artificial. Realmente yo tuve la oportunidad este año de ir al Google I.O. y ahí hablaron de inteligencia artificial hasta donde más les supo, pero hay un concepto que me llamó mucho la atención que es el Google AI Edge. Y en este video te lo voy a explicar. Sin embargo, hace poco creo que subí un video sobre el Edge Computing y está algo diferente. Entonces el concepto de Edge es procesar datos lo más cerca al usuario y entonces el Edge Computing es hacerlo a nivel de computador, a nivel de servidores, digamos de alguna manera. Google AI Edge es otra cosa, es procesar inteligencia artificial en el dispositivo. Vamos a ver de qué trata. Si buscas Google AI Edge, vamos a ver, vamos a encontrarnos con esta página y vamos a ver un poco acerca de qué es esto. Entonces aquí me dicen Google AI Edge. Dice On-Device AI for Mobile Web and Embedded Applications. ¿Qué significa eso? Básicamente, si nos basamos en el concepto de Edge, el Edge es procesar datos lo más cerca al usuario, Google lo que está aquí proponiendo es inferir modelos o correr modelos de IA directamente en los dispositivos. ¿Qué quiere decir? Digamos, en este celular yo podría correr literalmente un modelo sin necesidad de ir a la nube. Puedo hacer la inferencia directamente aquí. Acá tienen varias bases. Básicamente formas o tecnologías con lo cual lo pueden lograr. Google MediaPy, TensorFlow Lite, Gemini Nano. Luego vamos a hablar un poquito de Gemini Nano. Model Explorer. Y por acá de por sí nos dicen un poco de qué fue lo que se habló de esto en el I.O. Por ejemplo, acá hay algo muy interesante. No sé si es el Art Language Model en la web. Eso es súper, súper interesante. Vamos a verlo. Pero precisamente, ¿qué se refiere a esto? Normalmente, y vamos a irnos. Directamente a los demos. Vamos a probar los demos de Google MediaPy, que es una de sus frameworks o tecnologías. Aquí, por ejemplo, hay varios modelos y hay demos para hacer reconocimiento de objetos, clasificación de imágenes, gestos, reconocimiento de gestos. Muchas cosas muy interesantes. Pero lo interesante de todo esto es que Google MediaPy tiene una API que puede ser compatible con JavaScript, con Python, con Android y con Android. Entonces, yo podría correr estos modelos y hacer la inferencia directamente en el dispositivo. Entonces, son modelos que corren directamente on device. Vamos a hacer una exploración. Vamos a ver. Por ejemplo, aquí entre a uno que es el Object Detection. Entonces, aquí se está corriendo un modelo que en teoría está con la cámara detectando cosas. Entonces, por ejemplo, vamos a ver. Esto es un cell phone. Perfecto. A ver si reconoce qué es esto. Esto es... Esto es... Esto dice que es un cell phone. No, pero mira que ahí más o menos lo coge, ¿no? Bordle. Bordle, bordle, bordle. Ahí, ahí, ahí. Ah. Algo muy interesante de estos modelos es que lo hace en real time. Normalmente teníamos dispositivos como el Kinect, ¿no? Que lanzaba un montón como de luces como medio invisibles para detectar movimiento. Pero ahorita se está haciendo directamente con mi webcam y con inteligencia artificial. Con modelos de inteligencia artificial que son capaces de detectar cosas. Veamos, por ejemplo. Image Segmentation. Este, por ejemplo, es muy interesante. Vamos a ver si ya coge. Exacto. Porque me... Como que reconoce a la persona para saber en dónde está. Y esto es muy interesante. Fíjate que acá hay código en Android, en Python y en web para poder hacer este tipo de cosas. Y nos dice, mira, me entrega como un mapa en el cual yo puedo hacer cosas. ¿Qué se puede hacer con esto? Fíjate en esto tan interesante. Si buscas, por ejemplo, Google Meet y Google Media, en el que está el mapa, literalmente ellos están corriendo este tipo de modelos para segmentar, para, por ejemplo, reemplazar el background. Sí, literalmente aplicaciones como Google Meet o House, como tú los reconozcas, pueden reconocer la persona utilizando este modelo de segmentación. Fíjate, ahí reconoce. Y lo que no es soy yo, entonces lo quita y literalmente lo reemplaza logrando features como reemplazar el background en aplicaciones como Google Meet. Entonces tenemos aquí un poco cómo funciona el modelo de segmentación, cómo le ponen un blur, cómo me pongo en la playa, etc. Y esto es, imagínate que esto para lograr esto, pues literalmente estés lanzando o estés streameando en real time como tu cámara hace un servidor para detectar, ¿no? Como que esta inferencia podría ser demorada. En cambio, aquí lo que se propone es WebML o Web Machine Learning, que lo que hace es que estos modelos corren directamente. En la web, gracias a frameworks o gracias a tecnologías como Google MediaPipe, por acá tenemos MediaPipe, a WebAssembly, a tecnologías, por ejemplo, como WebGL, WebCPU, en donde todo ese procesamiento corre directamente en el navegador, en el dispositivo, en ese caso la web, mi dispositivo sería el browser, pero si yo tengo una aplicación en Android, en iOS, pues estaría utilizando el hardware del dispositivo para poder correr este tipo de modelos, entonces inferencia. Y aquí tenemos el botón de gestos, que es muy interesante, que se haga en real time, ¿no? Entonces yo me voy moviendo y él va sabiendo en dónde estoy yo. Miremos este, acá, el de gestos, me parece interesante, veamos que ahí se ejecute. Vamos a eso. Por ejemplo, esto es un TuneUp, que es como manito arriba, y vamos a hacer un TuneDown, vamos a hacer esto, no sé qué, Victory, según esto, esto es CloseFist, este como I love you, significa... Este, pero fíjate que entonces empieza, de por sí hay ya gente que ha hecho cosas como traducción de lenguaje de señas, utilizando este tipo de modelos, porque lo hacen en real time, ¿no? Y de nuevo, acá podemos encontrar documentación para hacerlo en web, fíjate que si yo abro este lib, me dan un, ni siquiera hay ningún framework, me dan un demo en TypeScript puro, acá, déjame bajo esto, donde literalmente utilizan un, ¿qué? Un par de... Un paquete que se llama MediaPyTagsVision, este es un modelo para visión, y acá tenemos HTML puro para lograr hacer el reconocimiento de imágenes, ¿no? Y tenemos todo el código para poderlo hacer, para poder usar un canvas, para poder dibujar, etc. Es un código que puede ser un poco complejo de entender, pero lo interesante es que si está en JavaScript, lo podemos acoplar a frameworks como para que corra en una aplicación en Angular, en una aplicación en React. Y es más, más interesante aún, si tú, por ejemplo, estás familiarizado con, con la aplicación de React, y tú, por ejemplo, estás familiarizado con, Tomás, con la aplicación de han Это仲 Dejamos estar, ¿te neighbourhooding también? Ja. MediaPy para correr estos tipos de modelos, esta inferencia directamente en el WebView, en el navegador, en este caso la inferencia se está corriendo en Chrome, en el caso de una aplicación en Ionic se correría en el WebView que da Ionic para la aplicación, pero en fin, se llena de oportunidades en la web precisamente, igual si lo quieres correr en una aplicación web de Angular o de React, fíjate que corre bastante bien, pero también lo interesante de esto es que corren offline, no necesitan de un servidor precisamente, podrían correr como una aplicación sin necesidad de conexión a un servidor externo, porque la inferencia se está haciendo en el dispositivo, entonces yo podría compilar mi aplicación que está escrita con Ionic, que está escrita con Angular, utilizar Google MediaPipe, la API para JavaScript y correrlo de forma offline, sin necesidad de una conexión a Internet. Veamos otra, a ver qué más hay por acá. Admar, veamos qué es este. Este creo que es muy similar. A ver, al de los gestos, había uno muy interesante, creo que es este, el de Synchronization, creo, que me pareció muy interesante. Creo que es Face Detection, a ver, déjame ver. Hay uno que te pone como la cara, bueno, este es Face Detection, me dice dónde está la cara de la persona. Ay, ¿dónde está el que yo quiero correr? A ver, voy a entrar aquí al Home y quiero correr, este es el que yo quiero correr. Sí, sí, era ese. Ah, mira, ahí, ahí, literalmente. Yo puedo cargarle un modelo 3D, hecho en, no sé, en alguna tecnología de 3D, y con ya el reconocimiento de cara. Entonces, por ejemplo, aquí les está hablando, ¿qué es esto? Un mapache. Un mapache les está hablando sobre Google MediaPipe y Google AI Edge, ¿no? Que son este tipo de tecnologías, este kit de tecnologías. Recordemos que no es solo Google MediaPipe, es TensorFlow, es otras tecnologías. Sin embargo, a mí me llama mucho la atención Google MediaPipe, porque, pues, precisamente lo podemos correr en la web. Me sigue causando risa que aquí estoy hablando, expresándome. Voy a ponerme muy furioso. Ahí estoy. Bueno, aquí como que sale mi barba, pero, en fin, son las cosas que se pueden hacer. Y esto normalmente era un poco complicado correrlo. Se necesitaba hardware un poco más especial, pero ahorita está corriendo con una webcam normal. Podría correr con la, como con la webcam de mi, con la cámara del dispositivo. Entonces, se corre o se ejecuta con base a modelos de inteligencia artificial. Veamos otro. Aquí hay otros que este, este yo lo utilicé y es muy interesante. Voy a poner permitir. Déjame ver si aquí tengo un micrófono. Estos son los micrófonos que tengo yo. Este, bueno, no creo que ya está tomando. Este es un modelo, por ejemplo. Voy a hacer también un ejemplo práctico, pero ahorita estoy haciendo como un overview de cómo funciona. Pero fíjate que ahorita estoy hablando y él me reconoce. No sé. No sé en real time qué es lo que está pasando. Entonces, me está diciendo que yo estoy hablando, que hay una persona hablando. Eso es lo que está pasando en ese audio en ese momento. Si me quedo en silencio. Reconoce que en ese momento había silencio. Si aplaudo. Por ahí reconoce como manos, ¿no? Además, si yo silbo. Vamos a ver si sale el del bar. Ahí está. Whistling, que es como silbar. Bueno, yo soy un buen mano silbando, pero se reconoce. La gracia es que mira que este modelo en real time iba a estar en la web y acá hay una prueba. Esto está corriendo en un navegador, no está infiriendo. Luego entonces yo puedo acoplar esto en mis aplicaciones y pues ejecutarlo. Se puede hacer clasificación de texto también. Aquí, por ejemplo, si bien sí se puede hacer clasificación de texto, yo confiaría más en modelos como Gemini. O modelos un poquito porque aquí utilizan un modelo de BERT que realmente ese modelo es bueno. Pero creo que ya hay cosas más avanzadas. O sea, los LLM, los Language Models ya tienen un como mucho más a favor. Pero si tú quieres un modelo liviano como para clasificar cosas como si algo es algo es. Pues bueno o malo, como acá dice esto es positivo, ¿no? I am happy. I am sad. Vamos a ver. ¿Qué pasa? Esto es negativo, ¿no? Como que por ejemplo para detectar reviews en una aplicación, ¿no? Como detectar el review y decir si fue un review negativo o no. Esto es un modelo que en teoría está infiriendo y ejecutándose en la web un modelo de BERT. También podemos hacer embeddings, language detection, que es como para saber en qué está escrito. Vamos a ver si detecta el español. Hola. Hola. ¿Para dónde va? No sé si detecta el español. Dice que es español. Y eso que estoy utilizando como, ah, una cierta cosa que se utiliza en Colombia que no es escribir para. ¿Para dónde va? Sino ¿pa dónde va? Digamos, ¿no? Pero al menos como que hay un 100% de que eso sea español, ¿no? Y hay algo muy interesante. Y lo que lanzaron en el I.O. Es que se puede correr un LLM. Los famosos LLM. También desde, directamente, desde el browser. Y para hablarlo un poco más me voy a acercar aquí a esta cámara. Y es, normalmente los LLM están tras una API, ¿no? Entonces tenemos a OpenAI, ¿no? Con sus modelos de GPT-4, GPT 3.5 Turbo, por ejemplo. En los cuales nosotros pagamos una key y pues por medio de una API que está muy bien distribuida, está muy bien. O sea, realmente los tiempos de respuesta son bastante buenos. Yo envío una respuesta y me dice, listo. Puedo hacer una conversación. Obviamente Google tiene también Gemini, Gemini Pro, Gemini 1.5, Gemini Flash. Que también están tras una API. Pero hay modelos y Gemini, bueno Google, está empezando a liberar cierto tipo de modelos de forma Open Source. Entonces, por ejemplo, liberaron Gemini Nano. Que es una versión como chiquita de Gemini. ¿No? Y esa versión Nano, como es Open Source, la pudieron comprimir. Que se pueda correr en TensorFlow. Y con Google MediaPipe correr entonces un LLM. Una versión de Gemini directamente en el dispositivo. Y de por sí vi muchas cosas de este tipo en el Google I.O. Que celulares como los nuevos Samsung o como los celulares oficiales de Google, como el Pixel. Literalmente van a tener estos modelos. Incluidos. Incluidos on device. Para directamente habilitar o por ejemplo si nosotros hacemos aplicaciones en Android. Directamente no tener que ir hasta la nube, hacer una inferencia, esperar una respuesta. Sino que podamos tener esos modelos directamente en el dispositivo. Y esto es una muestra. El Google MediaPipe precisamente es una muestra que yo puedo ejecutar ciertos modelos. Manipularlos, correrlos con JavaScript en real time. No es solo, mira, te envío una imagen, procesala. Y luego dime que, ¿cuál es tu respuesta? Fíjate que los demos que hicimos fue real time. Yo me iba moviendo, yo iba hablando, iba clasificando el sonido. Iba clasificando lo que iba apareciendo en la webcam en real time. Entonces el modelo de Gemma o Google está liberando o liberó Gemma Nano. Que es una versión de Gemini poco reducida. Pero que se puede comprimir y correr on device. Ahorita vamos a hacerlo, vamos a ver cómo funciona. Ya. Entonces lo primero que nos dicen es, mira, pues dime cuál es tu modelo. Y ellos tienen como un partnership con Kaggle. Entonces te dicen, mira, descarga el modelo de Gemma desde Kaggle. Tienes que registrarte y demás. Creo que voy a abrir mi sesión aquí rápido. Listo, ahí ya me lo guié. Normalmente necesitas de una cuenta de Gmail o lo asocias con alguna cuenta para la que quieras acceder. Pero una vez tengas tu cuenta aquí en Kaggle, te vas aquí a la opción de Google Gemma. Y por acá está. Y por acá está la parte de descargarlo. Entonces, por ejemplo, acá yo lo puedo descargar con Keras, PyTorch, Gemma con C++. Bueno, acá voy a descargar con TensorFlow Lite. Aquí me pide qué modelo quiero descargar. Hay varias versiones de modelo. Voy a descargarme esta primero. Y literalmente la descargo. Me dice que si la quiero descargar con el CLI de Kaggle. Pues claramente ahorita no quiero hacer esto. Entonces lo que voy a hacer es descargarme un archivo. Esperar la descarga. Y luego ya voy a poder seleccionar ese modelo que descargué. Y poder empezar a conversar con él. Porque al final un Last Language Mobile es hacer conversaciones. Con textos y demás. Entonces vamos a esperar la descarga. Que realmente pesa un montón. Este modelo pesa 2 GB. Hay modelos más pesados. Y ahí es donde hay algunas cositas que debemos tener en cuenta. En lo que descarga te cuento esas cosas. ¿Qué son esas cosas que debemos tener en cuenta? Por ejemplo, yo he hecho algunas pruebas precisamente de este Google Gemma. O Gemma Nano. Que pesa un montón. Y es, listo yo lo descargo. Lo hago parte de mi aplicación. De por sí, GitHub ya se muere por tener un asset tan grande. O sea, un asset. Porque se comporta como si fuera un archivo. Como una imagen. Como un JPG. Como un archivo CSS. Si yo lo pongo dentro de mi aplicación web. Dentro de las assets. Como jalo el modelo a mi carpeta de assets. Ya mi GitHub se explota porque es un archivo de 2 GB. Y te va a decir, oye, una de dos. O activas la versión de JIT. Que es como para archivos largos. O no sé si realmente está bien que nosotros. Como parte del repositorio pongamos ese modelo ahí. Normalmente allí habría que alojar esto mejor en un S3. Por ejemplo, que es para archivos estáticos. En algún servidor. Un content delivery network. Un CDN. S3 creo que está bien. Para que allí alojemos el modelo. Y luego simplemente con la URL y los permisos. Podamos decirle, mira, consume el modelo que está allí. Ok, pero una de las cosas es utilizar ese modelo de forma offline. Entonces, si al final estoy offline. Y yo quiero correr mi aplicación. Digamos que estoy haciendo una aplicación mobile. En Android. Con Ionic. Con tecnologías web. Con aplicaciones híbridas. Entonces, pues se pierde la gracia que yo tenga que ir a S3. Para pedir el modelo. Y que lo pueda ejecutar. La gracia es no ir a la nube. Entonces, precisamente lo que he encontrado para Android. Es que uno puede darle ciertos tipos de hacks a la hora de compilar. Para que permita que se compile ese archivo como pesado. Y sea parte de la aplicación. Sin embargo, ahí el trade off. O la desventaja es que tu aplicación quedaría pesada. O sea, tu aplicación no es solo tu aplicación. Si no es 2 GB de un modelo que tiene tu aplicación. Entonces, hay otras formas. Por ejemplo, he visto que en Android lo que hacen es descargarlo. No como parte de los assets de tu aplicación. Para que tu archivo. Para que tu aplicación no pese 2 GB más lo de tu aplicación. Sino que lo guardan como en la carpeta temporal del usuario. Como en el storage del dispositivo. No es parte del peso de la aplicación. Sino que al momento de iniciar la aplicación. Te piden como unos permisos. Y se descarga el modelo. Pero en el storage. Y con eso no hace parte del peso de la aplicación. En fin, he visto como varias cosas. Yo voy a hacer algún experimento con Ionic. Para ver como correrlo. Pero esas son cosas que hay que tener en cuenta. Pero es interesante que podamos correr estos modelos. De por si en un futuro lo que Google promete es que. De pronto ya ni siquiera necesitemos descargarlos. Sino con el SDK nativo de Android. Podamos inferir o correr estos modelos. Y decir, mira necesito a Gemma. Así como uno consume una API de Navigation.Geolocalization. Como uno consume el acelerómetro. Como uno consume. Pues la geolocalización. Que es una API nativa del dispositivo. Pues normalmente Android. Está empezando a habilitar ciertas APIs nativas. Para consumir modelos que ya están integrados. Y ya vienen dentro del dispositivo. Veamos, creo que ya se descargó. Entonces veamos como funciona. Entonces aquí ya se descargó. Vamos a ver acá. Vamos a ver aquí en las descargas. Vamos a descomprimirlo. Vamos a ver. Extraer. Extraer. Me lo descarga como un target Z. Eso normalmente es como un formato de Linux. Pero creo que acá Windows no puede descomprimir sin problema. Fíjate pesa 1.2. No pesa 2 GB. Pero si pesa 1.2. Es un. O sea para ser un modelo igual es pesado. Pero pues al final es un Large Language Model. Tiene bastante conocimiento de la vida. Y pues pesa literalmente. Entonces acá tenemos el modelo. Entonces ahora si ya deberíamos poder seleccionarlo. Como seleccionarlo desde la computadora. Aquí este es mi modelo. Ya lo cargué. Y yo podría hablar ya con este modelo. Que es una versión de Gemma. Y poder pues hablar con él. Entonces esperemos que cargue. Vamos a ver. Listo aquí ya cargó. Entonces puedo preguntarle hello. Vamos a ver. Hello. Listo. Hola. ¿De qué quisieras hablar hoy? Eh. Teach me Angular. Ahora como enséñame Angular. Entonces acá ¿no? Angular. Los. ¿Qué quieres aprender de Angular hoy? Como los fundamentos. El de. En fin. Fíjate que esto es un modelo. Y fíjate que la inferencia es bastante rápida. Eh. A ver. About testing ¿no? Quiero aprender sobre testing. Fíjate que la inferencia es muy rápida. Porque precisamente el modelo. El modelo. Está corriendo. No está corriendo. No está corriendo. No está corriendo. No está corriendo. No está corriendo. No está corriendo. No está corriendo. Está corriendo. No está corriendo. No está corriendo. No está corriendo. Porque este modelo no está corriendo. No lo está corriendo. ¿Por qué no lo está corriendo? Porque esto está corriendo. Este modelo está corriendo en la nube. Pero a veces. Entiendes, esto le está corriendo. Sí. Entonces dice que. Me está corriendo el mismo아니bol pieces hostnet. A ver. Oh. Eso. Él aquí doncuro. Ah bueno. Es lo mismo. O mirem. Se viejo forma de. Pero sé自己. Perdón. Pero la gracia. Es que estos testos. Es tu tipo de modelos. Cierran directamente. En el dispositivo. funcional, performance te lo voy a decir, unit testing I I prefer I prefer to learn about unit tests a ver y bueno, acá me empieza a hacer, creo que igual ya hay algunas cosas que ya empecé a perder contexto, como aunque mis preguntas no han sido tan buenas realmente pero me empezó a dar cosas de unit tests y de java y yo le dije que mi contexto era, quiero aprender angular y luego le dije testing y luego le dije unit tests pero pues esperaba que fuera unit tests dentro del contexto de angular pero de por sí, no es todo el poder el poder de por sí, todo el poder de ese modelo, pues es Gemini Pro que está atrás de estas APIs que te comentaba, pero es bastante interesante poder correr como una versión pequeña de estos modelos que ya se puedan correr directamente en el dispositivo listo, esto ha sido precisamente una introducción, más que a Google AI Edge, a Google MediaPipe que Google MediaPipe es como una de sus tecnologías más principales, pero Google AI Edge es eso que Google está empezando a habilitar para correr modelos directamente en el dispositivo que están accesibles precisamente con APIs para poder hacerlo en Python, para poderlo hacerlo en JavaScript, para poderlo hacer en Android, en iOS y podríamos utilizar este tipo de librerías de propuestas de Google para inferir y correr todos estos modelos recuerda que estos modelos, primero que todo una de las ventajas de correrlo offline o de las ventajas de correrlo on device es una de ellas es que es offline, no necesitas de ningún cloud, porque el modelo y la inferencia está ocurriendo en el device en la web ocurriría en el browser en dispositivos o en aplicaciones nativas pues ocurriría directamente en el dispositivo otra es la privacidad porque al final tú no le estás enviando esos datos o esa conversación o las fotos o tus videos directamente a la nube como a Google Cloud o a Gemini Pro o a OpenAI, sino que realmente como todo está ocurriendo en el dispositivo el modelo está en el dispositivo todos los datos son tuyos y salvas tu privacidad, esos son uno de los puntos también interesantes de lo que se conoce como el Google AI Edge o correr estos dispositivos estos modelos directamente en el hardware directamente en el dispositivo directamente en el navegador entonces cuéntame si sabías de esto, cuéntame si te interesa precisamente ver contenido hay varios modelos, vimos que Google MediaPipe como tecnología tiene varios tipos de modelos, el de reconocimiento de objetos audio correr Gemma Nano entonces cuéntame si estás interesado, yo quisiera hacer un video de uno por uno, ya integrarlo dentro de una aplicación en Angular, con Ionic por ejemplo para compilarlo como una aplicación en Android o en Ionic y ver cómo funciona, ver que si yo le pongo la cámara de mi dispositivo, haga la la detección, haga todo lo que nos muestra aquí en estos demos en la web cómo sería esto integrarlo a una aplicación en Ionic, Angular, compilarlo y utilizar esta propuesta de Google AI Edge déjame en los comentarios dale like si quieres que precisamente haga más contenido como este empiece a explorar directamente ya con código y ver cómo se haría este tipo de integraciones en tus aplicaciones en Angular y ya con código pues viéndolo real, ya ejecutando código, así que dime si ya sabías de esto, déjame en los comentarios si te causa interés, si ya sabías de esto no es la única forma, esta es la propuesta de Google, hay otras formas de correr AI directamente en el dispositivo pero estas son las librerías, esta es la propuesta que Google trae y está pushando bastante desde el Google I.O., así que déjame saber más al respecto si quieres más contenido sobre esto, así que nos vemos en la próxima