 En este canal he estado creando videos acerca del Landgraf y entendiendo como los fundamentos. Ya hicimos un sistema lineal de agentes utilizando el sistema de nodos de Landgraf. También ya vimos que es el Edge, el Conditional Edge, para decidir literalmente, dinámicamente, si irme por un nodo o otro. Pero hasta ahora no hemos integrado ningún Language Model y de eso se tratan estos sistemas. De integrar un Language Model que es como estos modelos generativos dentro de mi sistema de grafos. Y este es el video, este es el video precisamente en donde vamos a hacer esa integración. Así que, no siendo más, vamos al código. En este momento es donde vamos a unir Landgraf con Lanchain. Landgraf como sistema de orquestador de nuestro sistema de grafos, multiagentes, X. Con Lanchain que es como una utilidad, una herramienta para conectarse a las Language Models. Como de forma más sencilla o de forma agnóstica. Es decir, que yo puedo hacer relativamente fácil switch de un modelo a otro casi con el mismo código. Bien, cosa que no pasaría si tú utilizas native. Igualmente, por ejemplo, la LSDK de Mistral o de OpenAI o de LAMA. Normalmente ahí sí hay variaciones de código un poquito grandes. Pero Lanchain te promete un poco pues no hacer tantas variaciones. Vamos a verlo. Entonces, precisamente para ya conectarnos a OpenAI, que es como lo más común. Pero podemos conectarlo luego a otros. Vamos a ir haciendo los ejemplos. Pero lo que necesitamos es pues la librería. En este caso necesitamos Lanchain OpenAI. ¿Vale? Entonces, lo vamos a instalar. ¿Cómo lo instalamos? Vamos, recuerda que yo tengo Poetry como gestión o como el gestor de dependencias de mi proyecto. Es Poetry. No utilizo Pip. Pero si utilizas Pip, que es el manejador de paquetes oficial de Python, pues instálarlo así. Pero yo utilizo Poetry. Entonces, vamos a instalarlo con Poetry. Entonces, aquí voy a cancelar aquí mi agente. Y pues simplemente le hago Poetry. Poetry add. Y ahí es donde... Copio Lanchain OpenAI. Listo. Aquí me dice que ya lo tenía, porque sí, ya lo tenía. Pero pues simplemente ese es el paso que hay que seguir. Pues hay que instalar esa librería. Lo pueden verificar en el project.toml. Básicamente debemos tener esta librería. Ok. Entonces, lo siguiente es que necesitas una key. ¿Vale? Porque este modelo no está corriendo en local. Este modelo está en una nube. Está en los servicios de OpenAI. Entonces, necesitamos una key para poder acceder. Y ahí necesitas pagarle a OpenAI. Literalmente necesitas una tarjeta de crédito. Ahora, puedes utilizar Mistral también. Puedes utilizar Cloudflare. Puedes utilizar varios modelos que son open source. Ahorita voy a unirme a una OpenAI, que es ahorita lo más común. Pues para conectarse a una language model. En este caso, voy a conectarme al modelo GPT-4. ¿Vale? Ahora, yo necesito una key. ¿Dónde saco la key? Bueno. Bueno. Eso ya hace parte más de que tienes que ir aquí a la plataforma OpenAI. Ya configuras básicamente tu setup de tu proyecto. Y por acá, normalmente debe haber un ladito. Debe haber un ladito para generar las keys que siempre se me va. A ver. Do, Shaper Reference, Dashboard. Por acá tenemos las keys. Acá yo tengo algunas. Pero, pues, literalmente podríamos generar una key nueva. Entonces, acá voy a llamar demo. Esta key, solo para este video la voy a borrar apenas el video lo publique. Entonces, no la utilices. Básicamente es una key que simplemente voy a borrar después que termine esta transmisión. ¿Va? Entonces, aquí, esto es solo el ejemplo. Entonces, aquí me voy a generar una key. Esta es mi key. Y entonces, normalmente la pongo, pues, en mis credenciales. Esta también luego la voy a borrar. No se preocupen. No intenten robarme la key. Simplemente, ahí está. Va a ser de ejemplo. Luego las voy a eliminar. Listo. Entonces, ya tengo mi key. ¿Qué necesito? Algo que necesitan tener para cargar, pues, variables de ambiente es esta librería. Y literalmente necesitan esta. No es como, no, pero yo conozco otra que es más fácil lo que sea. No, necesitan esta. Porque el Landgraf, cuando quiere correr su sistema de agentes o el sistema de debugging, va a también a llamar variables. Variables de ambiente, si están desarrollando con Python, utilizando esta librería. Entonces, por favor, utilicen esta librería. Entonces, vamos a primero hacer como ese llamado, load en variable rules. Y vamos a hacerlo de esta manera. Entonces, acá, le faltó el from. Ahí está. .env. Acá tengo que seleccionar mi kernel. Ahí está. Agentes para que sepa, pues, en dónde estoy. Listo. Acá yo no me estoy conectando directamente a la librería de OpenAI. Recordemos que al final, .env. Lo vamos a hacer por medio de Landchain. Pero, pues, podríamos ver si sí cargó las variables de entorno. Al parecer, sí las cargó. Una forma que podríamos verificarla es imprimirla. No muy seguro, por cierto. Pero recuerden, voy a eliminar esta aquí apenas termine el stream. Entonces, voy a hacerlo, ¿no? Entonces, acá, por ejemplo, OpenAI aquí. Solo para que verifiquen que, pues, que sí. Que esta variable de ambiente que pusimos en este archivo, pues, sí la está leyendo. Entonces, acá puedo hacer un print y imprimirla, ¿no? Entonces, lo imprimo y ahí está. Está imprimiéndola aquí. O sea, literalmente hay una variable de entorno que se llama OpenAI API Key. Listo. Entonces, sí está. Sí la está leyendo, ¿no? Déjenme que acá se vuelva loco esto ya. Entonces, vuelvo a correr. Listo. Te voy a crear otro bloque de código. Y en este caso, siguiendo la documentación, vamos a importar este. From Landchain OpenAI. Fíjense que acá se crea la Language Model. Ahí está. Normalmente, se le puede mandarla aquí, como aquí por variable. Pero normalmente, si literalmente sigue en esta convención, o sea, si el nombre de la variable de entorno es así, por defecto la toma. Entonces, solo es cargarla, ¿no? Tienen que enviársela. Solo es cargarla. Listo. Y literalmente ya podríamos interactuar con OpenAI, ¿no? Ah, bueno, también podríamos decirle qué modelo quiero, ¿no? Modelo. En este caso, voy a elegir un GPT-4. Zero. Que es básicamente el que también tiene el poder de Vision, ¿no? Como de analizar imágenes, si es que yo se las envío. Entonces, aquí básicamente yo le voy a hacer Invoke. Esto hace parte de la API de Landchain. Van a ver que luego, precisamente la vuelta es que yo luego puedo cambiar esto por un Mistral. Y ya. Y luego puedo cambiar esto por un Lama de Meta, etc. Pero, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Y luego, etc. Bien. ¿Cómo? Yo integro esto a mi sistema de grafos. Entonces, vamos a literalmente crear un primer grafo. Entonces, acá vamos a crear el grafo. O mi sistema de agentes. Acá, básicamente sí necesito, ahora sí ya manejar mensajes. Bien. Entonces, por acá. Voy a saltar en esta parte. ahorita luego vamos a volver, pero acá nos dice cómo utilizar mensajes, básicamente lo que nos dice es que yo debería, recuerden que este es mi estado, yo podría agregarle ya un nuevo estado que es ya manejar los mensajes y en mi message sale, sale de la importación de Landgraf Core, mensajes, entonces ahora en mi estado de mi agente, pues no sólo está my bar, customer name, ahora hay una lista de mensajes, va a ser de qué tipo, any message, por qué, porque any message involucra, fíjense que acá ya me dice que involucra, involucra AI message, que es los mensajes generados por la AI, está el human message, está el chat message, el system message, el function message, el top message y bueno varios tipos de message, entonces él dice, oye necesitas crear un estado para los mensajes, para el array de mensajes y que se vaya guardando la conversación, bien, cómo utilizar un reducer, esto, me lo voy a saltar ahorita, porque lo reducer es más cuando yo quiero hacer el patrón de este, de que todos se llaman al mismo tiempo y luego tengo un aggregator o un map reduce para literalmente en paralelo se ejecutan, entonces necesito reunir la respuesta, pero este patrón no va a ser en otro vídeo en específico, entonces simplemente quiero llamarle la language model directamente, hay algo interesante y es que al final Landgraf es un framework, no? Entonces ellos ya tienen esto incorporado como de la mejor manera, entonces yo en vez de importarlo de esta manera, podrías importarlo directamente usando message state y literalmente hago una herencia, entonces básicamente lo que puedo hacer es esto y ya, entonces ya no tengo message, o sea, como que le agrego mi estado, que es my var y customer name, que es mi estado, pero al heredar de message state, ya sé que por defecto, eso tiene un estado para mensajes, bien? O sea, mi grafo va a tener un estado para mensajes, esta es la forma que yo lo recomiendo, literalmente porque al final es más, si uno va y ve el código, vamos a ver si me deja ver el código de message state, y creo que no me deja ver el código message state, ¿por qué no me deja ver el código message state? Quiero ir a la definición, ah, qué raro, una forma que pueden hacerlo, pues ir al report, ¿qué es un message state? Pero básicamente lo que tiene es esto, literalmente, lo que tiene es esta línea de código, es esa línea de código la que tiene, déjame ver si yo puedo hacer eso, esta es la línea de código que tiene, solo que pues como ya está incorporada, pues simplemente heredé de la clase que me da LandGraph, y pues ya listo, ya tengo como un estado que tiene un array de mensajes, está ya interno porque lo estoy heredando, bien? Listo, entonces acá, literalmente, este es el estado de mi gente, tengo los message, todavía no he conectado una cosa con la otra, pero sí ya tengo algo muy importante, que es, los mensajes, ¿no? Una, un, parte de ese estado es un array, en donde voy guardando los mensajes, ¿por qué esto es importante? Porque al final, para crear un flujo conversacional, yo necesito ir reflexionando sobre los mensajes anteriores, sobre el historial, para saber también qué decisiones tomar, ¿va? Entonces, listo, acá tengo, eh, message state, por acá nos dice, mira, message state, es, es un, un preview single message key, que tiene una lista de any message, y que usa add message, literalmente es esto, solo que ya he hecho, en, en esa clase, ¿vale? Listo, entonces vamos a ir al, al, al grafo, ¿no? Fíjense que acá, van a ser un grafo de un nodo, de un solo nodo, eh, y lo vamos a crear, literalmente, entonces, y ahorita no estoy haciendo, call functions, porque ahorita, quiero tocar ese tema de call functions, un poquito más, en detenimiento, entonces simplemente voy a hacer, un nodo, ¿vale? Entonces por acá voy a crear ese nodo, entonces acá lo voy a llamar, nodo uno, nodo uno, ya, nodo uno, sí, o nodo, nodo LLM, digamos, ¿no? Entonces acá tengo, eso, y acá, básicamente, tengo el estado, recordamos que esto debería siempre, eh, pues retornar un message stage, yo ya lo puedo llamar, de cualquier manera, yo, yo lo llamaría my state, o sea, porque no solo tiene messages, tiene, eh, tiene mi estado, sí tiene messages, pero también tiene otras cositas más, que le puse ahí, entonces, es más, yo, ¿cómo le llamaba? State, ¿no? State, yo lo llamaría State, o chat state, o agent state, lo voy a llamar State, me gusta más ese nombre, message stage no me gusta, porque significa que como que solo de mensajes, y no, realmente tiene mensajes, pero tiene otras cositas más, ahora, acá, básicamente, lo que hacen es, fíjense que acá yo tengo ya una instancia de, la language model, ¿no? Eh, con, eh, chat open AI, aquí es donde yo puedo ponerle la temperatura, qué tipo de temperatura quiere, cero, etcétera, ¿no? Por ejemplo, cero, que es como que no sea muy creativo, pero acá yo ya puedo llamar a la language model, invocarlo, y como estado, le envío el array de mensajes, es más, puedo imprimirlo, o sea, si yo hago un print, imprimo el estado, normalmente voy a tener, my var, customer name, y los mensajes, ¿vale? Y tengo el nodo, ¿no? Entonces, retorno los mensajes, retorno los mensajes, retorno los mensajes, va, perfecto, eh, también puedo hacer algo, puedo tener un system prompt, creo que por acá le agregan un system prompt, si no se lo agregamos ahorita, no hay problema, eh, voy a agregarle un system prompt, si no encuentro una por acá, eh, pues bueno, buscamos, si no hay un system prompt, lo agregamos, system, creo que no, no hay un system prompt, agregámoslo, porque normalmente es el que le dice, cómo debería comportarse, eh, mejor. Entonces, eh, vamos a crear un system prompt, eh, normalmente debería importarlo, de, de, de, de, de, de, de, de, de, de, de, ahí donde está esa importación, eh, human, a ver, vamos a ver si me ayuda, es que este human message no estoy muy seguro de dónde sale, si de, eso, quería ver, de langar code messages, ok, listo, va, va a salir de ahí, listo, entonces aquí, literalmente, eh, yo ya tengo la language model acá, pero, acá, en este nodo, vamos a importar message y acá lo que voy a llamar es un system message, system message, perfecto, entonces básicamente lo que voy a hacer es, siempre que yo llame a este nodo voy a crear un system message y por acá voy a llamar un system message, y digamos que el system message tú eres un helpful assistant, muy pobre ese system message, pero es un system message, entonces lo que puedo hacer es concatenar esto con esto, debería poderlo concatenar sin mayor problema, porque al final yo lo que hago es cuando lo invoco le mando todo el historial para que responda, y ya, debería funcionar, si no está mal, listo, ahora, vamos a crear el agente, porque al final sólo tenemos un nodo, entonces ahora creemos ya el grafo, entonces el grafo lo vamos a crear en esta iteración, déjenme ver si esto va bien, acá creé mi estado, todo bien, aquí creé mi node, la language model, me dice que todo corrió bien, entonces ahora sí es momento de construir el agente, ¿qué necesitamos para construir el agente? básicamente algo como esto, vamos a ver, acá, necesitamos agregar uno de los nodos, voy a llamar el nodo llm, y pues básicamente es igual a mi función node llm, no tengo ningún nodo más, literalmente no tengo ningún nodo más, ese es el único, no tengo ningún conditional edge, no tengo otro nodo como para pasarlo, por ahora simplemente va a ser un nodo, bien, y listo, y podríamos decir que el nodo llm apenas responda, va a ir o envía al final, o sea, simplemente, finaliza, bien, entonces, bueno, tú eres un helpful assistant, and your expert is in Angular, digamos, que eres un experto en Angular, digamos, digamos que eres un experto en Angular, digamos, muy pobre el prompt, pero de nuevo, son nuestros primeros pasos, listo, creo el estado, creo el nodo, es un nodo sin condicional, lo voy a compilar, vamos a ver si me compila, todo bien, me compiló, ahora vamos a generar la imagencita, para que veamos cómo crearía nuestro nodo, ahí está, es un nodo que va desde el start, node llm y finaliza, eso es básicamente lo que tenemos, vamos a probarlo, pero ahora para probarlo, ahora sí necesito salir del modo de notebook, y poner esto ya en un script, que yo ya lo puedo llamar en el sistema de debugging de Landgraf, entonces vamos a rápidamente poner esto en un archivo Python, entonces simplemente voy a crear acá, llm.py, vamos a llamar llm.py, y básicamente está dando todo lo que hemos estado experimentando, este no es necesario, ahorita vamos a ver que cuando yo ejecute, pues el lm.py, el forma de debugging de Landgraf, vamos a poderlo, él mismo va a cargar las variables de ambiente, entonces no hay mucho problema, simplemente entonces lo que necesitaría, es como copiarme estos elementos, entonces acá, este es el estado, ahí sí ya lo podría organizar, normalmente las importaciones siempre van como de primero, pues ya en algo convencional de Python, entonces aquí tengo este, que sería nuestro nodo, y este lo llevo, y este lo llevo, y este lo llevo, y este lo llevo, y este lo llevo, y este lo llevo, y este lo llevo, y este lo llevo, y este lo llevo, y este lo llevo, y este lo llevo, y este lo llevo desde acá, listo, y… aquí, quitamos este, este sería como la definición del nodo, de la definición de los nodos, por ahora solo tenemos uno, ¿qué más tenemos? Ah, ya, listo, tenemos el builder, y listo, y ahí está, y acá no quiero generar la imagen, porque pues ya, ya simplemente esto lo hago solo para ir debugueando como va quedando, pero acá ya quiero ejecutarlo, listo, este es, entonces, ahora si yo ya lo quiero debuggear, básicamente tendría que exportarlo aquí y literalmente le voy a poner un nombre aquí le puse llm voy a ese archivo y le digo que pues hay una variable llamada graph, que es esta de acá que ya está listo para compilar y pues responder, entonces ahora pues utilizando landgraf y su entorno pues voy a poner, voy a correr esto con landgraf-dev y pues vamos a ver si me lo, si todo va bien, por acá ya me está dando un error dice que no encontró la API client, ok dice que no la encontró, a ver déjenme ver si es que yo no le mandé ah, acá hay algo importante eso es lo que les quería mencionar acá por ejemplo en ningún momento le dije de donde lea las variables de entorno si yo estoy utilizando el sistema de debugging de landgraf tengo que decirle en donde están las variables de entorno entonces le voy a decir que las variables de entorno las cargue automáticamente haciendo referencia a mi archivo .emp que ahí es donde están mis keys, entonces le digo hey, en este archivo es donde vas a encontrar tus variables de ambiente, obviamente esto en producción cambia un poquito pero pues literalmente solo se tira la variable de ambiente en un entorno de Python pero para correr el sistema de debugging de landgraf, si necesitamos indicarle cual es el archivo entonces vamos a volver a correr landgraf-dev acá aparentemente ya no hay ningún error, ya me abrió aquí la consola vamos a ver acá fíjense que me abrió el nodo anterior pero es que fíjense que acá voy a tener una como un dropbox, no un dropbox, un dropdown para poder seleccionar cual agente es el que yo quiero correr este fue nuestro primer agente un agente de lineal solo para aprender el sistema de nodos pero ahora tengo uno que es el lm porque así le puse, recordemos que acá está el nombre lm, entonces voy a decirle lm y tenemos el mismo como gráfico que teníamos que nos generaba un poco aquí en nuestro notebook, este de acá pues un poquito más bonito pero pues es el mismo gráfico pero algo interesante es que tenemos messages y tenemos nuestro estado, tenemos tres elementos en nuestro estado, customer name bar name y messages en este messages me dice que mensaje, en este caso le voy a agregar un mensaje del humano y le voy a decir hola vamos a darle submit y ver que funcionó y por acá tenemos un error te dice launch prompt, debería ser un prompt value, bueno por acá al final creo que tenemos un error no sé si es por concatenar pero me dice que tengo un error ejecutando el chat prompt template déjenme ver que error cometí me voy a ir acá a quiero saber solo si acá donde le pusieron el nodo acá, si acá solo lo llamaron y déjenme ver si solo es porque le hice la concatenación del system message posiblemente solo sea eso ah si creo que es eso porque bueno ahorita déjenme probarlo solo para saber que esta concatenación debo hacerla de otra manera pero déjenme aguantar un momento solo voy a hacer este ajuste y si me funciona es porque ahí estaba el book vale entonces vamos a ir otra vez acá vamos a volverlo a correr acá literalmente puedo volverlo a correr voy a correr otro mensaje hola y si no es que se da concatenación le dice pues mal vamos a ver si responde ya no se debería responder tanto, digo state hola hola hola no respondió como yo creí que respondía se lo envió me envió, vamos a crear un nuevo hilo si es que de pronto el hilo anterior me está fallando ahí está ahora sí hola en que puedo ayudarte hoy fíjense que ahí está respondiendo voy a decirle como es experto en Angular le voy a decir cómo se crea un componente en Angular entonces voy a hacer un componente en Angular entonces lo envío bueno fíjense que ahorita no tiene el system prompt o sea literalmente este system prompt no lo tiene pero normalmente la language model o GPT-4 ya tiene conocimiento de que es Angular entonces él igual sabe resolverlo sin embargo de nuevo ya cuando creamos sistemas multiagentes complejos si necesitamos que no sea tan abierto que realmente si yo le digo esta cosa haga esta cosa, ahí es donde el system prompt funciona para guiarlo, para hacerle problem engineering, no hemos visto todavía code functions pero ahí es donde lo podemos extenderlo a nuestros sistemas pero al menos corre, como que corre me dio código etcétera acá por ejemplo yo puedo ver esta respuesta en JSON, es decir que yo puedo ver como este esto me respondería entonces si yo expongo esto en una REST API y hago este tipo de conversación pues básicamente tengo un array de mensajes que ya el frontend debería pues simplemente iterar y renderizar, por ejemplo aquí este content es exactamente esto y acá tiene markdown entonces obviamente nuestro frontend o el que esté pintando ya el chat debería leer o tenerle el feature de leer markdown para crear como estos códigos crear como estas este bonita forma de mostrar al usuario que acá literalmente me está también sugiriendo código y acá me dice crear de pronto le podríamos decir para saber si el system prompt está funcionando pues decirle que no me recomiende o sea que si le pregunto sobre react no responda, que tú eres de angular y solo enfócate en angular, esa es una buena guía de prompt genin, entonces vamos a ahora sí a ver cómo se hace el system prompt, porque no nos funcionó y déjenme ver creo que eso estaba en este ejemplo system, recuerden que programar no es no es no es como literalmente saberse todo de memoria, simplemente es saber buscar en muchos de muchas maneras, fíjense que acá tienen el system prompt, tienen el content y acá lo que hacían es que yo solo puedo hacer un plus o puedo hacer un merge de un array con otro array, entonces básicamente que es lo que estaba haciendo, veámoslo aquí en un notebook, básicamente lo que estaba haciendo es un string y eso que ni siquiera es un string porque un system message no es un string, es más veamos que es un system message, si yo le digo que es un, si yo imprimo este system message es un objeto, es un objeto que tiene un content y tiene un par de cosas más tiene content, tiene algunos parámetros, entonces es como un objeto, entonces básicamente yo estaba tratando de hacer concatenar un objeto con un array y esto pues me da error, es más aquí debería dar una especie de error, acá está tratando de unirlo pero no creo que sea la manera de unirlo, en fin, la forma correcta es unir otro array con otro array es básicamente la solución entonces al unir ese array con ese array pues si me genera como una un array de mensajes que lanchen puede leer bien, entonces vamos a ver si es eso o otra vez me vuelvo a fallar, entonces voy a iterar el system prompt, normalmente el system prompt va de primeras entonces esta vez ya no lo pongo directo como objeto, sino lo pongo en un array con un solo valor y lo concateno, recuerden que ese plus en python pues realmente hace un plus si ustedes hacen eso en javascript se les totea pero yo puedo hacer concatenación de un array con otro array simplemente haciendo lo demás y simplemente sumar los arrays, bonito, lindo, python en javascript toca hacer un concat o pues utilizar el split operation, en fin, no se puede usar un plus no se puede usar un más entonces vamos a ver si ya funciona con ese ajuste aquí ya le puse el system y le voy a decir acá, voy a agregarle only aquí me falta hacer más prompting este prompting tampoco debería ser el mejor only your responses your answers deberían ser en angular avoid avoid using other languages ok, sí, o other frameworks vamos a dejarlo así entonces es un asistente y tú eres experto en angular hay unas herramientas para mejorar el prompting por si acaso creo que hace poco vi una y tú le puedes mandar a este prompting que es súper malo, pero como que el te lo depura son herramientas de prompting pero ahorita no me voy a enfocar en el prompting, simplemente veamos a ver si este funciona igual estos nuevos modelos como GPT-4 normalmente se son mejorados este ya está un poco no lo estoy utilizando normalmente se adhieren más al prompt, normalmente en GPT-3 si era un poquito más complicado y como que tocaba darle mucha instrucción para que realmente no se saliera pero pero GPT-4 se adhiere un poco más al prompt pero igual podríamos hacer mejor prompting para que lo siga, ¿listo? entonces vamos a ver si me responde algo de React entonces listo, ya hice ese cambio y si el system message funciona bien primero no se debería totear, aquí no me no tengo ninguna falla, entonces si funciona mi system message, recuerdan que como lo estaba haciendo generaba un error ahora si estoy uniendo un array con otro, entonces todo bien y ahora veamos si se adhiere al prompt entonces vamos otra vez a estudio, voy a generar otro hilo y empezar de cero, entonces hola, vamos a decirle hola listo ¿en qué te puedo ayudarte hoy con Angular? eso está bien, o sea literalmente ya tiene una adherencia al prompt y me dice que como es un experto en Angular ya me dice, oye, yo soy experto en Angular pregúntame sobre Angular, no me preguntes sobre la vida, ¿bien? pero acá ya estamos viendo como final, como como funciona, ¿no? y vemos un poco más de cómo funciona nuestra gente recuerden que este end no es que finalice el sistema de agentes como tal, sino ese end es cuando ya le entrega una respuesta al usuario, que dentro de nuestro sistema dentro de nuestro sistema de multiagentes se puede iterar hasta realmente enviarle una respuesta al usuario, pero no es que se acabe la interacción, a veces podemos necesitar pues de una, por eso es importante el estado del message stage, porque necesitamos ir guardando la raíz de mensajes para de pronto con base al historial ir tomando cierta decisión, ¿no? como que cuando, eso lo hice por ejemplo en mi sistema de appointments como que cuando ya íbamos en un nivel del historial en donde el usuario ya me dio una fecha, una hora ahí si ya iba y llamaba un call functions para agendar la cita en el sistema de reservas de X empresa, pero eso es con base al historial ¿bien? listo, veamos preguntemos algo de React dime cómo hacer lazy loading en React, vamos a ver que me responde no sé que me responde, no estoy tan seguro porque el program no es tan bueno, pero pero GPT-4 de nuevo se adhiere bastante al PROM desde lo siento pero mi especialidad es Angular, si estás interesado en aprender cómo aprender, bla bla bla, y me queda aquí un ejemplo a veces yo le puedo decir oiga, no sea tan verboso o sea, como que se ve la respuesta tan larga, le puedo decir mantén las respuestas cortas, bueno ya es PROM engineering, ¿no? de nuevo entrando a otra vez a nuestro gran artículo de Anthropics sobre arquitectura y sobre patrones de la Language Model, ahorita sólo tenemos un nodo, podríamos decir que tenemos este, este patrón, tenemos sólo un nodo, es decir, hay un int, un start, hay un end o el out, y tenemos sólo un nodo pero luego podemos refinarlo pasarlo por varios Language Model calls pasarlo por refinamientos, etc depende de la complejidad de tu sistema de agentes, pero ahorita tenemos como el más base y literalmente con el que hice este artículo, y es un input con la Language Model y un output, literalmente aquí no hay mucho, pero si nos deja ver cómo empezamos ya a utilizarlo ahora ya yo ya puedo empezar a tener un nodo extra, ¿no? y es decirle ok, quiero un nodo más para hacer X, para hacer Y etc, etc, de esta manera ahora ya aprendiste cómo realmente se puede unir un poco el sistema de Landgraf como orquestador de agentes, como creador de sistemas multiagentes, con Langen, que es al final que se conecta con la Language Model funcionan bien juntos, pero realmente también pueden funcionar bien separados, Langen como una utilidad que simplemente la utilizas para conectarte a una Language Model sin necesidad de crear un sistema de agentes, o crear un sistema de agentes y no utilizar Langen, sino utilizar otra librería para conectarte a una Language Model, o directamente utilizar las librerías nativas como la de OpenAI, el SDK de Mistra, el SDK de Lama, etc las dos cosas pueden funcionar independientes, ahora a mi parecer pues funcionan muy bien juntas es decir, Landgraf ya como sistema de nodos, orquestador y Langen para conectarnos a la Language Model también hacer la parte de Call Functions, etc y exactamente en la parte de Call Functions voy a tener un video pero precisamente suscríbete a mi canal para que tengas ese contenido porque el siguiente va a ser literalmente pues ya tener un sistema de agentes más complejo, ya tenemos como los fundamentos, ese video te va a aparecer aquí, literalmente con este video puedes empezar a aprender los fundamentos de Landgraf luego, el siguiente paso es aprender los Edge, los Conditional Edge, que literalmente también lo aprendes en este video, que es la manera en que Landgraf ya decide como bifurcar una ruta y literalmente saber si tomo una ruta A o B para generar un sistema de bifurcación en un sistema de agentes y hoy, en este video, aprendiste como unirlo con la Language Model, teniendo esos tres videos como fundamentos ahora lo que queda es ahora sí crear sistemas de agentes mucho más poderosos que utilicen estos conceptos como para poder crear ya sistemas poderosos utilizando Call Functions o ya todo un Pylam de agentes o nodos que funcionen entre sí, así que si quieres más contenido como este, suscríbete y nos vemos en el próximo video.