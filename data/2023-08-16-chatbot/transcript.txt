 En este video te voy a enseñar cómo crear tu chat de inteligencia artificial conectándonos al LLM de OpenAI como API. Y vamos a hacerlo utilizando el reciente SDK que lanzó Verzel y que está muy bien unido a su framework de Nest. Así que vamos a empezar. En fin de este año, Verzel ha creado y publicado una librería para crear aplicaciones basadas en inteligencia artificial o que se apoyen en el uso de inteligencia artificial. Esta librería funciona muy bien con su framework muy conocido como Nest que básicamente es un framework construido sobre React. Y vamos a probar en este video cómo hacer un chat uniéndonos a un LLM, un Large Language Model, utilizando este SDK y cómo logramos crear un chatbot con todo este potencial. Así que vamos al código. Este es el artículo que te mencionaba en el cual Verzel ha publicado un SDK, una especie de librería, para consumir varios tipos de inteligencia artificial por medio de sus APIs. Entonces, nos podríamos conectar a la API de OpenAI para consumir modelos como chat GPT-3, 4, a Home and Face donde están modelos open source o también incorporar frameworks como Lancheng para hacer manipulaciones de un LLM de forma más compleja. Sin embargo, todo esto lo podemos orquestar con este SDK y entregar esos insumos a nuestra aplicación. Aquí hay algo y es que nosotros lo podemos hacer con frameworks en este momento con Nest sobre React y SvelteKit utilizando Svelte. Y muy pronto dicen que lo van a hacer con darle soporte a Nux para aplicaciones hechas en Vue. Y luego van a ir soportándolas posiblemente, y no sé realmente si, por ejemplo, van a soportar Angular, pero básicamente Verzel, pues obviamente está soportando a los frameworks que ellos soportan y mantienen, que en este caso es Nest y SvelteKit. Ok, entonces vamos a ver de qué trata y cómo nosotros vamos a construir una aplicación sobre Nest, es decir, una aplicación en React, pero pues utilizando Nest como framework y ver cómo funciona este SDK construyendo nuestro propio chatbot. Lo primero que vamos a hacer es ir a esta URL sdk.verzel.ai. Aquí hay un demo de utilizar ciertos tipos de prompts con ciertos tipos de modelos. Por ejemplo, aquí podemos ver el de Lama, el de chat.jpt, de OpenAI, etc. Pero nos interesa la parte de documentación en donde nos introducen hacia esta API o hacia este SDK. Y en este momento voy a construir un bot que utiliza OpenAI. Entonces vamos a irnos acá a OpenAI y vamos a seguir esta guía para construir nuestro propio chat. Y voy a explicar paso a paso qué es lo que está pasando en cada una de las cosas, porque aquí parece que fuera tutorial bastante corto, pero hay muchas cosas que están pasando detrás de esto. Así que vamos a empezar a crear esta aplicación. Aquí como ven, nos sugieren utilizar otro gestor de paquetes que no es NPM, sino PNPM. Sin embargo, yo voy a hacerlo normal con NPM, con Node. Pero si tú quieres seguirlo utilizando este gestor de paquetes o si quieres utilizar Yarn, también está bien. Así que vamos a crear primero la aplicación porque aquí también de una vez instalan el API de AI y OpenAI Edge y luego crean una key para poder consumir los modelos de OpenAI. Pero vamos a hacerlo paso a paso. Vamos primero a salir de la interfaz gráfica y luego sí vamos a hacer la integración de inteligencia artificial. Entonces vamos por nuestro primer comando en la terminal. Lo primero que puedes hacer es digitar el comando npx, que ese ya lo tienes integrado cuando tienes instalado Node. Y luego le corres el comando create next porque vamos a crear una aplicación de Next. Y le decimos que queremos una aplicación, bueno, es nextapp.arroba.layers, que serían como las últimas versiones. Entonces vamos a darle este comando. Aquí normalmente nos va a decir que si queremos actualizar este paquete. Le vamos a hacer que sí. Me dice cuál es el nombre de la aplicación, con Ness AI. Me dice que si quiero utilizar TypeScript, que si quiero usar Lindsay, que si quiero Tailwind, le voy a decir que sí. Y que si quiero utilizar el CDC como directorio. Y además si quiero utilizar el app router. Recordemos que Ness está haciendo una migración. Hace utilizar como una nueva arquitectura, sobre todo para hacer React Server Components. Y en este caso sería, vamos a utilizar esta nueva arquitectura que se basa en el app router. Entonces vamos a decirle que sí, que si queremos customizar el alias por default, le voy a decir que no. Y listo. Ese sería como el boilerplate por defecto que tenemos en una aplicación de Ness. Hay una librería además que voy a instalar que me va a facilitar la creación del chat. Si bien tenemos Tailwind, pues aquí voy a utilizar componentes que ya están escritos sobre Tailwind. Y sobre todo me va a servir mucho el componente del chat. Aquí hay un componente como de burbujitas de chat. En donde pues como que ya con un muy poco HTML y CSS, pues como que puedo crear este tipo de interfaz. Lo podría hacer con Tailwind puro. Sin embargo, pues la gracia es facilitarme el trabajo y pues simplemente hacer que me salgan estas burbujitas del chat. Así que voy a instalar esta serie de componentes que ya están escritos sobre Tailwind. Y vamos a ver cómo es la instalación. Realmente si estamos utilizando Ness, simplemente tenemos que instalarlo de esta manera. Y luego simplemente requerirlo como un paquete más dentro de nuestro archivo de tailwind.config.js. Entonces voy a copiarme este pedacito de acá. Y lo vamos a instalar. Además voy a copiarme lo todo. Y vamos a instalarlo también como parte de las dependencias que vamos a tener. Aquí ya terminó de hacer la configuración estándar. Entonces vamos a entrar a la carpeta que se llama chat.next.ai. Ahí está. Esta es la carpeta. Y recordemos que aquí vamos a hacer la instalación de esta librería de componentes sobre Tailwind. Entonces ahí está. Ahí ya lo encontró. Entonces vamos a ir al código. Vamos a entrar al código y ver cómo la configuramos. Estando aquí en nuestro proyecto, vamos a ir al paquete de tailwind.config. Aquí ya lo dejaron configurado. Acá está la parte de plugins. Y simplemente entonces vamos a hacer un recuadro de ese nuevo paquete. Ahora, haciendo eso, nosotros tenemos aquí una aplicación que tiene la estructura de una app router. Entonces acá tenemos el layout, la página. Y pues acá tenemos un hola mundo básicamente de una aplicación en Nest con el cual está utilizando Tailwind y como una configuración por defecto en la cual ya viene una aplicación de NestJS con React. A esta aplicación es a la cual le vamos a empezar a agregar, modificar y crear nuestro chatbot. Pero inicialmente vamos a ver cómo iniciar nuestra aplicación. Entonces podemos ir a nuestro terminal y dar un comando que sería el npm run dev. Yendo a nuestro terminal, entonces podemos correr nuestro proyecto que tiene TypeScript configurado, Nest, Tailwind. Vamos a ver entonces cómo abre por acá. Y listo, aquí tenemos como nuestro hola mundo. Y es en el cual vamos a empezar a trabajar. Como te digo, vamos a trabajar primero toda la parte de interfaz gráfica y luego vamos precisamente a integrar toda la parte del bot con inteligencia artificial, sobre todo para entender qué tanto es lo que nos va a ayudar esta nueva SDK. Qué tanto trabajo nos va a ahorrar tanto en el UI y también en la parte de backend. Así que pues empezamos a hacer la maquetación de nuestro chatbot utilizando las clases de Tailwind. Lo primero que voy a hacer es borrar toda esta sección. Es decir, no necesito nada de esto. Vamos a borrar todo esto y luego entonces vamos a empezar a utilizar ciertas clases. Entonces aquí básicamente voy a utilizar varias clases de Tailwind para hacer la maquetación. En este caso es más, voy a borrarlas para identificar cuáles son las que estamos usando. Entonces primero le voy a decir que este layout va a ser tipo flex, que va a tener un mínimo que sería un mínimo de la altura de la pantalla. Le voy a decir que igual es flex, pero que todo lo mantenga como en forma de columna. Si te fijas, cada una de estas clases al final significa algo dentro de CSS. Es como una transformación, una forma más corta. Pero bueno, básicamente pues por eso estamos utilizando Tailwind. Y si ustedes quieren ver que solo compararse en la clase les dice pues cuál es la traducción de CSS o la propiedad de CSS que se está utilizando, recuerda que puedes utilizar el plugin de Tailwind dentro del Visual Studio Code. De por sí creo que lo tengo por acá. Vamos a ver cuáles. Es este que tengo por acá. Tailwind. Es este que tengo acá. Si lo quieren instalar de esa manera. Así que partiendo de ello es que estoy haciendo esta maquetación. Entonces le voy a decir que los elementos van a ir centrados. Recordemos que aquí estoy maquetando rápido en cuanto a Tailwind, porque no quiero enseñar Tailwind, quiero hacer el chatbot, así que en otra oportunidad veremos Tailwind más a profundidad. Así que simplemente estoy haciendo el maquetado. Estoy agregando un padding de 24 píxeles y que todo su contenido esté con esta esté justificado básicamente. Listo. Entonces básicamente aquí le dije que tengo flex y que va a ser en forma de columna. Entonces yo quiero mantener los mensajes arriba, es decir, estos van a ser los mensajes. Y por aquí va a haber el input de texto en donde vamos a escribir. Normalmente esa es la interfaz de un chat. Normalmente en la interfaz van mensajes y luego un input como abajo en el cual vamos a escribir. Listo. Aquí va a haber algo muy interesante y es que ya puedo utilizar componentes que había traído de esta librería. Entonces casi que voy a copiar y pegar ese código. Vamos a verlo. Entonces aquí vamos a buscar la librería de chat. Debe estar por acá. Es esta burbujita. Ahí está. Acá voy a por si me da el código en el formato JSX. Entonces casi que lo puedo copiar y pegar para crear para ver esta interfaz de chat como se ve. Entonces la puedo copiar directamente y la puedo llevar aquí a mi código. Listo. Fíjense que aquí hay unas imágenes que obviamente no tengo en mi local. Entonces las voy a tener que sacar de algún lado. Que sería como la foto del usuario. Entonces para no complicarme con asset en este momento voy a utilizar Random Username que es una API para Random Username para usuarios. Este de por acá. Ahí me genera como fotos de usuarios de forma randómica con una sola URL. Y voy a elegir pues estas URLs de los usuarios. Entonces aquí está. Voy a copiar la imagen y voy a irme a mi editor y voy a poner la imagen de forma directa. Listo. Entonces esta va a tener una. Esto para no tener ni tener que descargar assets ni nada. Sino simplemente pues sería la forma de tener una imagencita ahí puesta. Fíjense que acá sale un warning. Ahorita vamos a ver por qué. Básicamente es porque no estoy utilizando la etiqueta image de NEST para hacer la optimización de imágenes. Pero listo. Básicamente tenemos aquí un div en donde pues tenemos los mensajes o se van a renderizar los mensajes. Y luego también un div en donde se va a renderizar un textarea que es básicamente en donde vamos a escribir. Ahora hay que tener algo en cuenta. Y es que en NEST para renderizar imágenes que vienen desde otro lado toca incluirlo dentro de el NEST config. Y aquí también hay un protocolo como él a veces hace optimización de imágenes. Bueno, aunque aquí estamos utilizando image, entonces posiblemente no sea requerido. Pero si utilizamos image, pues nos va a tocar colocar esa regla de imágenes. Primero probémoslo aquí. Vamos a ver cómo funciona así. Y luego vamos a ver que utilizando image nos toca colocar un protocolo para decirle que acepta imágenes que vengan desde este dominio. Ok. Entonces vamos a ir y ver cómo funciona en este momento en nuestra aplicación. Listo. Y aquí tenemos en nuestra aplicación simplemente son burbujitas allí en donde pues me está diciendo los mensajes de cada uno de estos usuarios. También podemos ver que aquí podemos con diciéndole chat start o chat end pues como mandar un mensaje a un lado o al otro. Es más, fíjate que acá sale el nombre, la fecha y si fue entregada o no. Esos ya son elementos que tú puedes agregar o no, pues de acuerdo a las necesidades. En este momento solo voy a hacer que el chat como de mensaje de un usuario aparezca un costado y el del bot aparezca el otro. Entonces voy a utilizar el end. Voy a irme a mi código. Fíjate que simplemente es poner este en end y pues lo que venga del usuario, lo voy a poner en end. Normalmente va en ese lado y lo que venga del bot sería start. Entonces vamos a ver cómo está. Ahí está un poco nuestra aplicación. Podríamos mantener como un avatar para saber que este es como el usuario. Ahí está. Y pues vemos básicamente que acá tenemos a nuestro usuario y este sería nuestro bot y esa sería como nuestra interfaz. Listo. Tenemos como una interfaz base para mostrar como los mensajes. Recordemos que aquí queremos utilizar más que todo inteligencia artificial y no tanto preocuparnos por la UI. Así que ahorita vamos a maquetar rápidamente obviamente también nuestra área de texto y luego sí empezar a utilizar pues esta API. Sin embargo vamos a hacerlo de la optimización de imágenes porque pues buenas prácticas. Entonces vamos a utilizar el image, image, image. Ahí está. Le voy a decir que tiene la propiedad fill, que es que llene todo su pues el espacio necesario de acuerdo al padre. Y en este caso entonces, bueno, vamos a ver que por acá me dice que necesito un alt. Eso está correcto. Entonces este sería el user. Este también sería nuestro user. Y por acá en nuestro alt habrían mejores alts, pero pues por ahora vamos a dejarlo así. Entonces fíjate que por ejemplo si yo trato de correr esto me dice, hey, no puedes utilizarlo por lo que te decía de que hay una restricción en los dominios. Una vez ya utilizas esta etiqueta image. Así que vamos a ir a next config y vamos a agregar esta etiqueta. Entonces aquí vamos a ir a image. Vamos a decirle a remote patterns. Ahí está, remote patterns. Y ahora vamos a decirle las reglas que vamos a soportar. En este caso le vamos a decir cuál es la URL que vamos a soportar. Normalmente también se puede escribir en forma de objeto, entonces le puedo decir que el protocolo sea por HTTPS. También el hostname. Le vamos a decir que el hostname en este caso nuestro dominio es random username, entonces lo ponemos ahí. Y un path, o sea si hay un path en específico, nosotros le vamos a decir que pues cualquier path nos va a funcionar. Listo. Entonces ahora ya debería funcionar. Vamos a ver si recargamos. A veces si hay algunas configuraciones en este archivo, en el next config toca matar el y volverlo a encender, pero Ness ha trabajado bastante en eso y como que ya no toca hacerlo. Fíjate que aquí tenemos un issue y es como el borde, como que ya no aparece en forma circular los avatares. Entonces le voy a poner esta propiedad mejor a la imagen. Entonces aquí vamos a hacer un class name y le agregamos esa propiedad a la imagen. No la tendría el div, sino la imagen directamente. Entonces vamos a ir por acá. Si tú normalmente mi contenido también tiene que ver y está enfocado en Angular porque también me gusta mucho Angular como desarrollo. Entonces si tú vienes de Angular y estás aquí chismoseando un poquito de también cómo se ve Ness, esta imagen también existe en, o esta directiva para optimización de imágenes, también existe en Angular, que es el ngRCE. Y casi que funciona muy similar. Tiene una propiedad field, hay un protocolo para ver cuáles son los dominios o si vamos a utilizar un cdn de imágenes que esté en otro lado, etcétera, etcétera. Listo, aquí ya agregamos ese issue, ya tenemos una optimización que es utilizar image y también el dominio que vamos a habilitar para esto. Ahora vamos a irnos a la parte importante que es la que nos va a recibir el input. Entonces vamos a decir que esto es un formulario. A este formulario vamos a decirle que pues tiene un textarea, textarea, y un botón de enviar. Listo, y el botón pues como es un formulario vamos a decirle que es de tipo summit, para que envíe, y pues ponemos un botón de enviar. Listo. Entonces voy a eliminar esto y pues vamos a ya agregar algunos elementos pues que ya tiene React. En este momento voy a procesar precisamente esto con React. Entonces yo necesito un evento, también necesitariamos maquetarlo, maquetemoslo primero. Entonces esto sería flex, que ocupe todo el ancho de su padre, entonces sería un full, y luego pues que aparezcan en forma de columna, esto sería un flex-col, y que haya un espacio en ye, entre cada elemento. Entonces un espacio de dos yo creo que es suficiente, y yo creo que ahí está bien. Este como es el padre, deberíamos también ponerle que ocupe el width deseado. Al textarea podemos ponerle como también algunas clases. Y por si hay una clase que tiene Tegwin para esto, vamos a buscarlo por acá en textarea, que como que le agrega ya ciertas propiedades. Entonces podemos ponerle este, me parece que está bien. Listo. Textarea border, vamos a poner un placeholder para decir como escribe aquí, escribe aquí. Y vamos a ver como va quedando. Entonces por acá tenemos, acá está nuestro botón, todavía podemos mejorar. Acá vamos a ver nuestro botón, botón, botón, botón. Acá tienen unas propiedades también para un botón que pueden ser bastante útiles, solo que toca encontrarlo. Entonces vamos a utilizar el search, botón, ahí está. Y también tiene ciertas propiedades, entonces voy a decirle que es un botón primary, entonces voy a utilizar estas clases. En esto se parece mucho por ejemplo a frameworks como Bootstrap, en donde pues cada clase ya tiene alguna representación, pero está utilizando o re-usando básicamente Tegwin por ejemplo. Entonces vamos a ver como va quedando nuestra interfaz, ahí está nuestra interfaz. Vamos a ver qué más podemos hacer por nuestra interfaz. Por ahora también podríamos empezar a mejorar, fíjense que como que los mensajes no ocupan todo el ancho, pero pues eso lo podemos ir mejorando. Por ahora, podríamos dejarlo ahí, simplemente como actividad porque queremos empezar a utilizar inteligencia artificial. Luego ya lo hemos maquetado, lo podemos mejorar. Entonces ahorita me voy a preocupar más por recoger lo que me escriban en el text area, para luego enviárselo a la API y utilizar el SDK de inteligencia artificial. Entonces aquí voy a crear un estado para guardar el text area, entonces, o lo que escriban en ese text area. Entonces vamos a ver que voy a crear un estado aquí, un input y por ende subset input, ahí está. Entonces eso sería un useState, que básicamente viene de React, entonces lo importamos de React, ahí está. Y también vamos a tener una función que va a manejar el evento de pues, de lo que es el envío. Entonces aquí tendríamos un evento y pues al final esto es una arrow function. Esto es un evento y vamos a decirle que ese evento es de tipo for even, porque estamos utilizando TypeScript y es buena práctica pues seguir haciendo typing de todo. Y aquí lo que vamos a recibir es los elementos, un HTML for element. Listo. Aquí lo que voy a hacer es prevenir, hacer un prevent default para que no haga ninguna recarga al enviar el formulario, porque eso es un comportamiento normal que tiene HTML. Y luego pues simplemente imprimir el valor del input. Ok. Ahí estaría nuestro valor del input. Acá hay algo que también no hemos utilizado, o sea no hemos asignado este handle input, ni tampoco hemos utilizado el set input. ¿En dónde se utilizan? Entonces vamos a asignarle a nuestro form que sería un on summit, que es como el que envía. Le vamos a asignar precisamente y específicamente ese evento. Y ahora pues el valor del textarea podríamos decirle que el value es igual a el input, lo que haya en el input. Input, input. Input. Ahí está. Ahí está. Y ahora vamos a ver cómo el se, pues deberíamos registrar el cambio. Cada vez que haya algo, pues debería actualizarlo. Y de por sí para eso es donde utilizaríamos el set input. Básicamente lo podríamos hacer de esta manera, un check. Cada vez que cambie, pues también podríamos leer un evento como change input, lo voy a poner. Vamos a ponerlo como change input y puede recibir el evento, etc. Fíjense que acá también ya me está haciendo el typing, entonces vamos a decirle que nos vamos por ese typing. Y lo que vamos a hacer allí es utilizando un set input, vamos a poner aquí más que current target, debería ser target. Ahí está. Vamos a ver que por aquí me da un valor, aunque quedamos con el current target para ver cómo lo opera y si sí funciona. Entonces vamos a ponerlo por acá, change input, y pues lo que tendríamos que tener es que cuando le demos enter, cuando le demos enviar, pues deberíamos tener el valor del input en este formato. Vamos a ver si funciona. Aquí, por ejemplo, hay un error muy interesante y es que useState son cosas que funcionan desde el lado del cliente y realmente en la nueva arquitectura que te comentaba de NEST, hay componentes que se renderizan del lado del servidor y otros que se renderizan desde el lado del cliente. Es mal aquí, él me dice, oye, si quieres hacer renderizado desde el lado del cliente, pues marca ese componente con useClient, es decir, esta sería la forma de marcar en Next, sobre todo en X13, utilizando AppRouter, que este componente corre del lado del cliente y no del lado del servidor. Una vez dado el useClient, entonces vamos a ir aquí a abrir nuestro inspeccionador, nuestra consola. Yo estoy utilizando Microsoft Edge para utilizar el motor de Bing y su inteligencia artificial, pero pueden utilizar al final Chrome, porque al final utilizan Chromium por detrás, es la misma cosa. Entonces vamos a ver si yo escribo aquí hola y envío el sent, ahí está, tengo el estado, es decir, estoy teniendo el estado de ese campo. Ahí está. Lo podría como limpiar, ¿no? Como después de recibirlo. Pero es lo mínimo que necesitamos para empezar a implementar la inteligencia artificial. Entonces ya tenemos nuestra parte de interfaz, que por si aquí ya también te entregaban algo ya un poco maquetado, que es muy parecido a lo que tenemos, pero vamos a ver que aquí utilizan un hook que viene de un lado, pero necesitamos entender qué es lo que hace este hook. Entonces lo vamos a ir entendiendo más adelante, pero ya tenemos nuestra interfaz. Ahora necesitamos instalar estas dos dependencias, AI y OpenAI Edge, que básicamente es la forma también de que vamos a consumir a través del SDK de Grursel al OpenAI. Y fíjate que acá también nos toca crear una key en OpenAI para poder consumir sus modelos de LLM. Entonces vamos a hacer primero la instalación y luego crear esa key. Para hacer la instalación, simplemente vamos a hacer un EPM install y instalamos estos dos paquetes, AI y OpenAI guión Edge. Entonces vamos a hacer la instalación. En este momento estoy utilizando MPM, pero tú puedes utilizar el gestor que más te guste. Ahí ya está instalado. Ahora vamos a crear nuestra key para consumir OpenAI. Para hacerlo, tú tienes que tener una cuenta en OpenAI y entrar a platform.openai.com Aquí en tu cuenta personal vas a habilitar aquí tus API keys. Aquí tengo que hacerte una aclaración y es la siguiente. Para poder consumir estos tipos de modelos, sobre todo de OpenAI, que te ofrecen consumir los LLMs con modelos como chat GPT-3 Turbo o chat GPT-4, necesitas estar registrado y consumir. Y ellos te van a cobrar por consumir esos modelos de inteligencia artificial. Por ejemplo, ahorita ya tenemos modelos OpenSource como LLama, pero aún así también tendrías que tener un servidor en donde desplegar LLama y, por ejemplo, consumirlo ya desde tus servidores. Como aquí vamos a utilizar los modelos de OpenAI, necesitas habilitar esta facturación y por ende necesitas una tarjeta de crédito porque esos descuentos van a llegar allí. Así que ten mucho cuidado con lo que gastas allí. Y aquí también te voy a hacer una recomendación. Realmente no es tan caro, o no es muy caro, realmente ahí están los precios de cuánto te cobran por cada token, porque al final estos LLMs cobran por token, pero si también lo vas a utilizar ya de forma empresarial, es decir, quieres hacer tu chat para una empresa, para un caso de negocio, pues tienes que tener esto en cuenta porque básicamente va OpenAI te va a cobrar por utilizar estos modelos. También si al final tienes muchos usuarios, o sea, si vas a hacer un software para miles y miles de usuarios, aquí también te recomiendo otra cosa, que no utilices OpenAI, porque OpenAI no soporta tantos requests. Pues como que tú haces muchos requests y luego te va a banear por hacer tantos requests en poco tiempo. Entonces para una aplicación que tiene millones de usuarios, entonces no va a ser tan fácil y pues puede que te baneen. Entonces allí, por ejemplo, te recomiendo más que utilices la API de Azure, Azure OpenAI. Recordemos que Microsoft y OpenAI tienen una alianza y si quieres utilizar esto para millones de usuarios, ya sería mejor que estos modelos los consumas, no directamente desde la API de OpenAI, sino desde la API de Azure, en donde también están los modelos de OpenAI, pero ya de forma más enterprise y disponibles a millones de usuarios si quieres desarrollar un chat que va a estar expuesto a miles y miles de usuarios. Si son cientos de usuarios, pues no hay problema, puedes utilizar la API de OpenAI y punto. Si quieres más información, pues ya puedes ir a Azure OpenAI Service y puedes investigar más acerca del servicio, sobre todo porque también ya te, aparte de que no tiene un límite tan corto como el de OpenAI, pues tiene también algo muy interesante como seguridad y demás. Y pues básicamente son aplicaciones que escalan a millones de usuarios. Ok, entonces ya estando aquí en las keys y teniendo tu facturación activada y demás, entonces vamos a crear una nueva key, aquí le voy a llamar chat o Nes chat, NesGs, creo, no, NesGsChat. Y aquí también tienen que tener mucho cuidado con sus keys, es decir, cuando creen una key, aquí me está dando la llave con el cual yo puedo consumir la API de OpenAI con modelos como chat GPT-3 o chat GPT-4. Y es muy, tienen que cuidar que esta llave no se filtre en internet, porque pues una vez se filtre, cualquiera puede utilizar esta llave y al final tú estás pagando por esta llave y pues puede que te lleguen consumos muy caros. ¿Qué quiere decir? Que esta llave solo va a estar disponible por este demo, por este video, pero pues pronto lo acabe, lo voy a eliminar y pues ya nadie va a tener acceso a esta llave. Entonces, no te molestes siquiera en utilizarla, porque simplemente no te va a funcionar. Crea la propia tuya y cuídala muy bien que no se filtre, no la subas en ningún repositorio, porque si se filtra, pues básicamente cualquiera la puede usar y puedes utilizar cualquier modelo de OpenAI y tú al final estás pagando esa facturación. Entonces lo vamos a copiar y fíjate que acá en el tutorial nos dicen que esa llave tenemos que meterla en un archivo .env. Recuerda que el archivo .env no hace tracking en GifCup, es decir, es un archivo ignorado en GifCup, entonces no va a quedar en nuestro repo. Para eso son esas variables de ambiente. Entonces vamos a crear ese archivo en nuestro proyecto. Acá, nuestro .env .env Ahí está. Fíjate que acá está y debería estar ignorado .env Vamos a ver por qué no me lo está ignorando. Si está en verdecitos es que no me lo está ignorando. A ver si puse bien el nombre del archivo Sí, realmente es .env A ver, vamos a ver, vamos a colocarlo acá. Lo otro es que igual esta llave la voy a borrar, entonces pues si igual se llega a filtrar en el repositorio no debería haber ningún problema Entonces vamos a ir acá, vamos a decirle entonces vamos a copiar, bueno, vamos a copiar desde acá vamos a ir por acá y pegarlo ahí y ahora sí vamos a copiar esta llave Listo Esa sería mi llave Acá está nuestro .env. Qué raro que me lo estoy trackeando de por sí aquí dice que, ah bueno, tiene un .env local, que también lo lee nest aunque también debería ignorar el .env normal, entonces, pues debería ignorar ese y cualquier .env Vamos a decirle que también ignore cualquier .env, fíjate que ahora sí me lo ignoró y no solo el .env.local Aunque realmente creo que nest sí también si tú le pones .local igual te lo ignora, ahí está ignorado y vamos a ver si funciona con ese .env.local, sin embargo con seguridad es mejor ignorar cualquier .env que es básicamente lo que hicimos acá en el GTI le agregamos una regla para que ignore cualquier .env Ok, entonces aquí ya tenemos nuestra llave, una vez le hagamos don, ya queda ahí y fíjate que ni siquiera la puedes volver a ver, entonces también tienes que copiarla y simplemente pues ya agregarán tus variables si no la copiaste en ese instante, no la puedes editar realmente, o sea le puedes cambiar como el nombre pero no puedes volver a ver el contenido de la llave, tendrás que eliminarla y volver a crear otra, ok, listo entonces ya esta era como la esa parte compleja, ahora vamos a crear, fíjate que aquí utilizan otra habilidad de nest, que es que dentro de nest no solo podemos hacer frontend, sino también algo de backend hasta APIs entonces aquí vamos a utilizar el rower handler para crear la API la cual vamos a consumir y esto ya corre en el lado del servidor es backend, va a consumir las APIs de OpenAI Edge, va a consumir el token, acá está el token y va a hacer el request utilizando un modelo como chatgpt 3.5 turbo, entonces pues vamos a empezar con esta parte de la implementación para hacer esa implementación lo que vamos a hacer es dentro de app vamos a crear una carpeta, un folder nuevo que se llame API, en API vamos a crear una solo para entender un poco como funcionan estos routers, entonces voy a poner una carpeta llamada hello, y a hello le voy a agregar un archivo que se llama route.ts typescript, ok aquí voy a importar algo de next que se llama next server entonces vamos a poner un paquete de next server y aquí quiero utilizar el next response listo, una vez hecho esto voy a crear un holamundo un endpoint que es un holamundo entonces vamos a crear esto de forma asincrona esto es una función, aquí me la creo como middleware pero realmente aquí tenemos que poner la función, esto sería un get entonces pues simplemente lo ponemos de esa manera y es más voy a hacer el response de forma directa entonces aquí hacemos un return es más por aquí me está contestando que puedo responder en plano lo cual es una mentira, debería responder en json entonces vamos a responder en json y le digo una propiedad como nombre John Doe listo fíjate que aquí el routing tiene o como llamemos a las carpetas tiene mucha importancia fíjate que acá lo llamamos a api hello y esto es un get, significa que yo puedo ir al browser el 3min donde corre la nes ir a api hello y como es get entonces allí debería tener mi respuesta hacia mi URL que no sé por qué no está renderizando en este momento pero vamos a ver por qué tenemos una falla allí vamos a ver creo que tenemos nuestra aplicación abajo porque habíamos instalado nuestras dependencias entonces pues vamos a volver a correr entonces aquí deberíamos tener nuestro hola mundo, ahí está un endpoint literalmente hecho con nes y aquí es donde podemos implementar toda la parte de backend, es más hasta podríamos hacer una un consumo directo a una base de datos para dársela a nuestro frontend, entonces aquí es donde nes también está haciendo algo muy interesante que es estas líneas a veces divisorias que hay entre backend y frontend, las está haciendo en un solo framework, es un framework full stack, normalmente tenemos a angular por ejemplo por un lado o a view o a react por un lado haciendo nuestro frontend y luego tenemos una api en otro lenguaje por ejemplo python o en nes con s, recuerda que esto es un framework para hacer este nes que es diferente a nes con z y nes con s este nes también es un framework para hacer apis del lado del backend pero aquí nes lo que hace es hacerlo todo en uno, entonces aquí mismo estamos haciendo nuestra api, hay muchas discusiones si esto es bueno, si es malo, si deberíamos seguir teniendo las divididas o no pero pues simplemente esta es la api que nos ofrecen en este momento y por ende de por si por eso vamos a construir y vamos a utilizar este SDK porque lo podemos correr desde el lado del servidor ya está viendo un poco como funciona entonces pues básicamente tenemos aquí este endpoint y es más lo podemos consumir también por cualquier sistema para consumir apis, por ejemplo postman o yo utilizo mucho insomnia, entonces vamos a abrir insomnia y pegarle a este endpoint entonces vamos a esperar que abre insomnia ahí está abriendo insomnia listo, entonces aquí tengo precisamente, bueno por si esto es un poquito de spoiler, pero vamos a crear un nuevo request http request, vamos a decirle que quiero consumirlo le envío un pues un request al endpoint y aquí tengo la respuesta entonces vamos a ver en el código que si yo lo cambio y le hago un nuevo request, pues ahí está entonces tenemos una respuesta y esto es una ruta o un endpoint hecho directamente desde la aplicación vamos a ver el código vamos a ver el código, acá está este es el endpoint, este endpoint lo voy a dejar ahí quieto porque pues vamos a construir un endpoint directamente ya para nuestro chat, bien es más en el tutorial nos dicen que queremos esto en chat route, entonces vamos a crear esa carpeta y el archivo route, entonces vamos a crear chat y vamos a crear route.ts perfecto listo, vamos a copiarnos un poquito como este template que tenemos acá para saber un poco como es que vamos a irlo haciendo y en este caso le voy a decir que lo que yo voy a atender es por POS bien, va a ser de un request de tipo POS porque vamos a recibir información que sería el mensaje de nuestro usuario y luego vamos a empezar a procesarlo ok, entonces aquí vamos a hacer algo interesante que es lo siguiente, vamos a importar es más eso me lo puedo copiar del tutorial para no darle tantas vueltas, aquí vamos a importar esta configuración esta de acá aquí ya estamos importando vamos a importar de OpenAI Edge, que Edge es una una tecnología bastante interesante y una forma muy interesante de correr frontend y API desde el lado del servidor, es un poquito de serverless pero con mucho más alto nivel, pero luego hablo un video de hablar solo de que es estas tecnologías de Edge, entonces pues vamos a ver, por ahora pues simplemente esta API va a soportar como en este tipo de tecnología de por si por este runtime es importante, decirle que el runtime va a ser de tipo Edge y fíjate que aquí es donde cogemos la configuración, la configuration de OpenAI, le pasamos nuestra i, que está leída desde las variables de entorno y pues esa es la configuración inicial ahora fíjate lo siguiente ahora nosotros vamos a leer entonces vamos a leer ese request ahí llega un request y los mensajes van a llegar aquí, van a llegar una serie de mensajes que normalmente llegan en el body y pues los vamos a extraer de esta manera listo, esos son los mensajes que llegan, ahora fíjate que aquí hay un response vamos a ir a hacer response y aquí voy a comentar esta parte del stream porque quiero ver explicar esto un poquito más a fondo, entonces voy a decirle que todavía no quiero stream y nada que vamos a utilizar un modelo que sería el GPT 3.5 turbo, me imagino que acá hay cosas como la temperatura, yo puedo aquí saber que temperatura tiene el modelo o como voy a configurar la temperatura del modelo etcétera, etcétera, estos ya son hiperparámetros que tienen los modelos y que pues ya deberías estudiar cuáles son los hiperparámetros de cada uno de los modelos para saber como tu chat puede contestar o no la temperatura básicamente se refiere a que tan libre o creativo se va a poner a la hora de crear la respuesta, si yo le pongo uno, como que va a ser rudo y va a tratar de ser muy estricto en su respuesta, pero si yo le pongo nueve va a ser bastante creativo pero puede que empiece a inventar por satisfacer la respuesta empieza a alucinar y a inventar información entonces hay que empezar a jugar con los hiperparámetros y con otras técnicas para por ejemplo reducir o consumir información de una fuente más confiable o dejar que el modelo realmente invente todo etcétera, por ahora esos son temas que vamos a entrar a ver a pues hablarlos un poquito más a fondo en otros videos, pero por ahora dejémoslo por ejemplo con una temperatura muy creativa de un 0.9 listo, ahora fíjate que aquí yo le quité esta parte que es lo del stream ahorita vamos a responder que es eso del stream, pero por ahora fíjate que acá la data voy a guardar esa respuesta, ese procesamiento la data la voy a hacer utilizando una wait. response.json solo que sin este stream response, aquí estoy utilizando copilot por eso como que me está a veces autocompletando código pero pues a veces como que falla o no, por ejemplo cuando me autocompleto plaint aquí y plaint no existe es una función que no existe porque le está tratando de a veces adivinar cuáles son las funciones que puede llamar, pero esto ya es que estoy utilizando copilot pues dentro de mi entorno de Visual Studio Code ok, entonces fíjate que acá tenemos la respuesta y pues yo podría imprimir directamente y pues decirle data listo, entonces aquí vamos a ver que es lo que estamos haciendo, fíjate que acá consumió a chat completion ahora realmente la API que estamos utilizando que vamos a consumir es esta, chat completions de por sí podemos ver el código en Node que es básicamente lo que estamos haciendo desde Nest, en donde se puede crear y aquí es donde se mandan los mensajes nosotros le dijimos que los mensajes iban a venir pues por POS, pero recordemos que los mensajes tienen que tener una estructura los mensajes son requeridos, tienen que tener un rol, el sistema usuario, asistente o si es una función, el contenido el nombre de la función si es que al final vamos a tener un function call, etc. Entonces vamos a tener que enviarle ese formato no podemos enviarle cualquier cosa porque si no falla y es muy importante de por sí aquí en las buenas prácticas si uno realmente quiere sacarle mucho provecho a un modelo como el que tiene OpenAI tiene que saber muy bien como utilizar el system el user, el assistant porque eso puede cambiar mucho como funciona el comportamiento de nuestro bot así que vamos a hacer nuestro ejemplo utilizando el formato adecuado. Entonces ya tenemos nuestro endpoint, entonces ahora nos toca pues clonarlo y por acá por ejemplo yo tengo la forma en enviar estos mensajes, así lo vamos a enviar entonces fíjate que aquí ya tengo un endpoint un poco preparado en donde vamos a consumir el API chat y aquí fíjate como se le envían los mensajes los mensajes tienen que ser un array la cual tiene que tener un role, un contenido y si es un mensaje del usuario pues user y luego el contenido y él va a tratar de completar esa respuesta. Entonces aquí en sistema es donde yo le puedo decir cuál es el role del asistente en general, por ejemplo aquí yo le dije que tú eres un tutor de JSSenior pero le puedo decir tú eres un tutor de Python por ejemplo, de Python Senior y luego pues cuando una pregunta ya viene del usuario entonces lo marco como user y por ejemplo pues le hago una pregunta como ¿qué es algo en Python? ¿qué es un iterador? iterador como yo le dije que era un senior en Python pues él va a entender un poco el contexto y me lo va a, espero me lo conteste en lenguaje Python ¿no? un iterador en Python, etc etc. Entonces fíjate que si yo le envío el request aquí me va a tardar en procesar los LLMs pueden tardar en procesar no son tan instantáneos fíjate que pues aquí ya está tardando un poco más de 10 segundos y tenemos la respuesta tardo como 12 segundos la respuesta pero fíjate que acá me da la respuesta me puso un ID, me dice cuando lo creo, qué modelo fue el que estamos utilizando y aquí fíjate que me devuelve la respuesta en un role en el role que sería un asistente y aquí me dice un iterador en Python es un objeto, en fin esta respuesta ya es creada directamente por el modelo interior de Python y esta es la respuesta que le podemos devolver a nuestro frontend y pues simplemente renderizarla entonces vamos a hacer ese trabajo para hacer ese trabajo fíjate que él va a recibir messages por ende aquí tienen que tener las instrucciones del role del sistema para entenderlo entonces aquí en page vamos a enviar ese role del sistema entonces vamos a crear un role y aquí vamos a enviar entonces vamos a crear un nuevo estado, en este caso voy a ponerle que son los mensajes por ende aquí es un set messages vamos a decirle que por ahora es un array por ahora puedo usar typing un poco más extremo por decirlo de alguna manera es decir que un mensaje tenga un role que tenga un content, etc. pero por ahora voy a decirle que sea un array de cualquier cosa no es la mejor práctica pero simplemente voy a entrar en ese detalle en este momento ahora le voy a decir que el mensaje inicial precisamente va a ser como con el que inicia el estado por defecto pues va a ser el de el role del sistema el role del sistema es el que me dice como pues él se va a comportar y yo se lo puedo programar desde acá, recordemos que era content entonces voy a ponerle que el role del sistema era que pues se debería comportar como un Python con Signior en Python y listo ese es el comportamiento por defecto que quiere decir que cuando escribamos un nuevo mensaje deberíamos como apilarlo a esta línea de mensajes y luego mandárselo pues precisamente a el backend entonces pues vamos a hacer eso precisamente una vez tenemos el input que sería como la propia del usuario básicamente podríamos construir algo llamado new message que tiene que tener pues un role ese sería el role del usuario y luego el contenido que en este caso es nuestro input, ese sería el nuevo mensaje ahora ese nuevo mensaje yo lo podría pues unir a una raya de mensajes ese es el nuevo mensaje entonces ahora voy a tener new messages en donde pues voy a copiar básicamente los mensajes anteriores y le agrego el nuevo mensaje creando como un nuevo array, bien, una vez haga eso voy a decirle que voy a limpiar el input, se lo voy a dejar en vacío aparte de eso voy a modificar set messages para que ahora tenga una nueva lista de mensajes y una vez ya tengo el array preparado pues se lo tengo que enviar a nuestra API entonces aquí básicamente tendríamos que crear otra función send to send message to API básicamente que es asíncrona, si, en eso está correcto, es asíncrona y lo que vamos a hacer es recibir esos mensajes, es más aquí está autocompletando un poco como debería enviar ese fetch pero pues vamos a hacerlo despacio, entonces aquí que es lo que tenemos aquí básicamente lo que deberíamos es recibir los mensajes que al final ya dijimos que era por ahora un array de any, de cualquier tipo y entonces hacemos un request, vamos a hacerlo utilizando fetch y aquí figurate que como la aplicación corre en el mismo puerto entonces simplemente tengo que decirle API chat y le digo que va a ser por tipo post porque pues al final recibe post entonces vamos a ir por acá y tenemos el método post ahora del cuerpo del mensaje que al final serían los mensajes, recordemos que nuestro endpoint al final recibe los mensajes que es un poco también lo que hicimos aquí, los mensajes y él nos va a contestar pues de esa manera, ahora pues una vez hecho esto pues vamos a responder entonces esto como es un fetch entonces lo podemos guardar en response y decirle que es await, esperar la respuesta response y esa respuesta pues aquí tenemos la data entonces la procesamos la respuesta de esa información, ahora en donde viene la respuesta, fíjate que la respuesta viene en un array llamado choice y básicamente en el primer nivel de ese array pues está el mensaje del asistente entonces deberíamos coger ese pues este mensaje y agregarlo a nuestros mensajes en local básicamente lo que podríamos hacer es volver a hacer un set de los mensajes y podríamos tener la data previa e iterarlo, entonces set aparte de recibir un array directo también puede recibir una callback function con su estado anterior, esa sería la data previa, entonces yo le podría decir que tenemos la data previa y aquí no le puedo iterar directamente la data, recordemos que al final eso está dentro de choice entonces podríamos decirle que quiero data.choice vamos a hacer que esté bien, en la subcero y allí pues quiero el mensaje y listo entonces el mensaje message que tiene error y con perfecto, aquí podemos añadirle, vamos a ver si esto aquí me deja proseguir puedo decirle que esto es any para que no me moleste en este momento ahorita vamos a ir viendo como solucionamos este tipo de percansas con el typing, pero por ahora pues debería funcionar sin mayor problema ah por acá tengo un error lo que no era necesario ponerle era simplemente un error de type, listo listo entonces fíjense que acá una vez ya pues tengamos los mensajes pues debería pasar todo esto borrar el input y refrescar nuestra interfaz porque recuerden que tenemos el mensaje del usuario, luego le mandamos eso al backend básicamente a este endpoint y el endpoint va a responder ya al mensaje del asistente y lo va a agregar a los mensajes que sería pues el array que tenemos acá, entonces tendríamos tres mensajes, el del sistema el del usuario y luego el del asistente cuando ya nos responde entonces nos faltaría iterar, ¿cómo iteramos esto? esta iteración es bastante sencilla porque al final pues deberíamos iterarlo de esta manera pues ya desde JSX cogemos la messages y empezamos a hacer la iteración entonces, ¿qué vamos a iterar aquí? pues lo que vamos a hacer es iterarlo de esta manera, vamos a literalmente imprimir aquí la burbujita entonces la imprimimos de esta manera cada uno de los elementos tiene que tener uno aquí, entonces vamos a ponerle que sea index y básicamente fíjense que de acuerdo al role deberíamos decidir si imprimirlo con start o end por ahora voy a imprimirlo todo con start simplemente para mirar que todo nos funcione y aquí es donde va el contenido entonces acá ya tenemos el message bueno realmente si tenemos message, message.content que es donde está el contenido y estas dos burbujitas las podemos quitar, listo vamos a ver cómo funciona, listo aquí tenemos la interfaz, fíjate que el primer mensaje que es el mensaje del sistema se imprime luego vamos a crear un mensaje de tipo usuario hola, dime cómo hacer un for un for fíjate que no le voy a decir en qué lenguaje como ya tiene ya el chat tiene un preset que es el del sistema que se debe comportar como un python senior pues él ya debería tener el contexto y responderme en ese contexto vamos a ver que me responde, fíjate que la respuesta pues ahí llega y por aquí tenemos un error parece que no no procesó choice en el sub 0 y ahí tenemos el detalle, vamos a ver que puede ser lo que nos está fallando entonces ya nos tocaría empezar a debuguear esperámonos una forma de debuguear es fíjate que como que llega la respuesta entonces vamos a ver qué onda con la respuesta, qué hay en la respuesta para ver en dónde estamos cometiendo el error entonces vamos a ir aquí al console y estar pendiente de la respuesta, hola cómo hacer un for vamos a ver aquí tenemos la respuesta y fíjate que acá nos dice que hay un error dice que message es requerido y tiene toda la razón porque nosotros no le enviamos los mensajes, o sea recuerda que los mensajes tienen que estar así message y un array dentro de un objeto y nosotros no lo enviamos así, nosotros lo enviamos directamente, así, message como array entonces esto tiene que ir en un objeto llamado message, entonces ese era el error entonces vamos a ver que ahora cómo hacer un for send vamos a ver, aquí tenemos el request fíjate que se está demorando, tienes que tener muy en cuenta eso, los LLMs no son tan rápidos, entonces hay que empezar a hacer ese procesamiento pero aquí tenemos la respuesta para hacer un bucle en Python puedes usar la sintaxis, etc. y nos da aquí código que nosotros ya con algún plugin en React o en cualquier framework que estemos usando, podemos imprimir estos pedazos de código como ejemplos de código que nos está entregando, entonces por acá puedo hacerle otra pregunta, a ver acerca de Python ¿cómo funciona la concurrencia? bueno, ¿cómo funciona la concurrencia? recordemos que él ya sabe que es un experto en Python, entonces debería respondernos con ese preset que ya le entregamos al sistema entonces vamos a ver la respuesta ahí se está demorando y ahí es donde precisamente vamos a construir o utilizar el stream, porque ahorita le vamos a dar la posibilidad de que como los LLM se demoran mucho en procesar, entonces lo vaya contestando en un streaming de datos, pero aquí tenemos la respuesta, fíjense que acá me dio una respuesta bastante larga, la concurrencia es la capacidad del sistema aquí, ah bueno, acá en Python, o sea creí que me lo había contestado de forma genérica pero no, aquí ya me dice mira en Python puedes utilizar el módulo tal para crear y administrar hilos, entonces fíjate que me está siguiendo como la configuración que yo le hice al sistema, ahora suena raro que la configuración del sistema aparezca al usuario el usuario debería ser transparente o sea tú le das como una forma de comportarse a tu tutor y pues el usuario no debería saber cuál es esa forma que le diste en el cual se comporte, entonces quisiéramos filtrar y que sólo aparezcan mensajes del asistente y mensajes del usuario, pero que no esté el mensaje del sistema y como eso tiene un rol pues básicamente podemos añadir un simple y bonito filtro y decirle ok mira sólo imprímeme los mensajes, acá tengo los mensajes en donde su rol sea o usuario donde sea igual igual a un usuario o el message role sea assistant assistant ahora vamos a ver si lo escribí bien, ahí está assistant entonces el mensaje del sistema que es el que le dimos aquí no va a aparecer entonces vamos a ver entonces yo recargo todo, fíjate que está el chat como en vacío porque pues el mensaje del sistema no está hola ¿cómo estás? vamos a ver que me contesta sólo con esto con un ¿cómo estás? sí hola estoy bien, fíjate que acá me dice pero estoy aquí para ayudarte en tus preguntas sobre Python, ¿por qué? porque yo ya le dije que él se tenía que comportar como un señor en Python, aquí es donde esto ya se puede empezar a a complejizar un poco más y hacer Prom Engineering y es empezar a ver cómo realmente tenemos un buen Prom del sistema y hay muchas técnicas muy interesantes que se han creado alrededor de esto para realmente darle un muy buen contexto, inyectar contexto, inyectar bases de datos, inyectar muchas formas para que realmente se comporte de una manera deseada esta es una manera muy simple pero aún así miren que funciona sólo con darle este comportamiento al sistema pero hay otras formas de hacerlo más complejo y que realmente siga tu forma estructural en la cual quieres diseñar tu experiencia para tu usuario ok, aquí casi que ya lo tenemos, o sea literalmente ya está funcionando, pero aquí fíjate que nos tocó hacer varias cositas como que procesar hacer, etcétera hay algo que precisamente el SDK de Vercel nos va a ayudar pero primero quería que entendieras todo este proceso todo lo que se estuvo haciendo porque casi que en en el backend esto se queda así sólo vamos a añadirle lo del streaming pero en frontend nos ahorra todo este código a cero utilizando una librería que viene ya desde el AISDK precisamente porque viene preparado para frameworks como NEST o como NOX entonces vamos a ver todas esas líneas de código como se reducen utilizando la librería, si volvemos a nuestro tutorial fíjate que aquí la parte de la interfaz él no creó tanto código, él simplemente utilizó este hook lo vamos a copiar lo vamos a llevar, vamos a utilizar este hook que viene dentro de la librería AI React, use chat y acá están todas las funciones que nosotros habíamos creado manualmente entonces fíjate que está está la ray de mensajes ok, ya no lo necesitamos, ¿por qué? porque simplemente él ya lo va a gestionar por nosotros, el input tampoco lo necesitamos, el handle input change era nuestro change input que es el que actualiza el estado del text area el handle submit que es el que envía esa información, tampoco la necesitamos, pero entonces ¿cómo le decimos que envíe esto a IP chat y además de eso que este sea el comportamiento del sistema? pues esas son variables que recibe el use chat entonces aquí le podemos decir, oye la API a la que vas a ir es API chat y hay unos mensajes por defectos unos initial initial message que son un array y aquí es donde le podemos poner pues el mensaje inicial que sería el del sistema que es el role, aquí te va a pedir un ID, obligatoriamente cada mensaje va a tener, tiene que tener un ID entonces le voy a poner que sea getTime aunque aquí me lo pidió en formato string entonces voy a poner un formato string que sería podría ser un get iso vamos a ver to iso este, este me funciona tan, listo listo, entonces ya borraríamos toda esta lógica, toda esta lógica que hicimos nosotros se borra, y ya y aquí cambiaremos nuestro handle, habíamos puesto change input por el handle input change y fíjate que ya no necesitamos de esa lógica todo eso, toda la lógica que habíamos hecho, simplemente ya la recibe de esta manera, y vamos a habilitar finalmente el streaming, aquí el streaming igual a true, y vamos a cambiar un poquito este response, porque si ya lo tenemos que hacer de forma de streaming, él nos dice que deberíamos convertir ya no esto a JSON sino a un streaming y retornarlo de esta manera ahora el streaming para que va a ser recuerdas que el chat se demora, se demora, no es tan rápido, a veces 10 segundos, a veces 13 segundos, y pues que el usuario demore tanto en la respuesta pues puede ser algo complejo, pues aquí le estamos diciendo al chat que vaya escribiendo a medida que va procesando entonces, hola, como hacer hola, para que sirven los comentarios, digamos, a ver que me dice, y como recuerden como ya lo preconfiguramos para que tenga un comportamiento de Python Senior, pues vamos a ver como nos contesta y fíjate que él automáticamente me está escribiendo y esto es por habilitar el streaming entonces fíjate que aquí ya la respuesta es casi instantánea no tenemos que esperar 10 segundos simplemente habilitamos el streaming y aquí utilizando use chat, él ya sabe que tiene que ir recibiendo esa respuesta e ir renderizándola con nuestras reglas de negocio, por ejemplo, esta regla se sigue respetando solo debe imprimir los mensajes que son de tipo role y asistan, y podemos poner una regla más, y es si es del bot lo manda a un lado, si es del usuario lo manda al otro, entonces esa regla la podemos hacer aquí, entonces esto sería un string dinámico como es un string dinámico entonces lo vamos a usar de esta manera entonces básicamente la pregunta podría ser un if abreviado en donde le digamos hey, si el mensaje si el role es igual o igual a asistan si es igual a asistan entonces es un chat end pero si no, pues es chat start y aquí me faltó algo del if abreviado chat start entonces vamos a ver si si funciona y ahí tenemos que uno fue a un lado, otro al otro ya tocaría ver las fotos de los perfiles pero pues ya tenemos nuestro chat funcionando, vamos a contestarle algo más en bot, aquí también podríamos decirle como un poco como que sea más resumido se responde en un párrafo es que está contestando muy lejos en un párrafo vamos a ver en un párrafo, ya también podemos darle muchas otras reglas de por si aquí ya te invito a la documentación de OpenAI tiene muchas buenas prácticas para ver OpenAI pueden entrar en DocsAPI aquí, y aquí hay unas buenas prácticas de realmente cómo hacer mejores prompts para que todo tenga mejores resultados entonces vamos a ver acá es más como una introducción dejame ver, ascender request, modelos chat, hay una parte donde donde nos daban como buenas prácticas, acá está GPT best practices y acá nos dicen ahí tienes que enviar prompts de esta manera el sistema está configurado de esta manera y hay muchos ejemplos como insertar contexto bueno, hay muchas maneras en cómo hacer realmente muy buenos prompts para que el bot se conteste de una manera muy interesante, entonces pues vamos a ver ok, y ¿cómo hago? para crear ¿qué? framework framework me recomiendas para iniciar vamos a ver qué me dice iniciar el desarrollo de aplicaciones web para mí sigue siendo muy largo o sea, como que el párrafo sigue siendo muy largo, pero bueno eso ya son técnicas de prompt que ya podrías empezar a ver, pero aquí ya construimos un chat que tiene una respuesta en streaming y en donde procesamos utilizando el Vercel AI SDK pues la construcción de un bot utilizando y consumiendo los modelos de GPT de OpenAI bueno, y esto ha sido todo por este video, hemos construido un bot desde cero, desde literalmente la creación del repositorio hasta todas las configuraciones que tuvimos que hacer para que realmente funcione nuestro bot como ven, los pasos son pequeños, sin embargo quería explicar paso a paso desde la UI fíjate que hicimos todo el código de RIAC para saber precisamente cuál era la parte que nos ahorra utilizar el SDK de pues el Vercel AI SDK que es donde simplemente utilizando un hook pues ya nos ahorra la parte de procesar los mensajes, el estado del input, enviar eso al pues al input que creamos, etc. pero es importante que sepas cómo funciona y cuáles son esos trabajos que te estaría ahorrando este SDK falta hacer nuestro deployment y aquí te aconsejo que hagas el deployment pues utilizando Vercel, precisamente ese OpenAI Edge es para que funcione en los servidores de Vercel, así que puedes empezar a probar este tipo de cloud, aunque también lo podrías desplegar en tus servidores personales, pero realmente funciona también muy bien en los servidores directamente o en la cloud de Vercel, entonces dime si quieres hacer más contenido respecto al tema, utilizando Inteligencia Artificial y Frontend, así que nos vemos en la próxima, recuerda compartir este video a muchos de tus amigos compañeros de los cuales quieren empezar a utilizar Inteligencia Artificial y están utilizando Next y React para crear aplicaciones, pues aquí Vercel les trajo una muy buena solución integrada, esperemos que lo empiecen a abrir a otros frameworks, posiblemente se puede utilizar en React y en Svelte, lo quieren abrir hacia Vue con Nuxt y esperemos a ver si llega a frameworks como Angular así que nos vemos en la siguiente hasta luego