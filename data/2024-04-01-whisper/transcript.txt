 Utilizando Whisper, un modelo de inteligencia artificial creado por OpenAI y que además de eso es open source, vamos a poder transformar audio a texto, sobre todo en el transcript de ese audio con alta precisión. Y todo esto con Python, así que vamos a verlo. Este, Whisper, es el modelo de inteligencia artificial que creó OpenAI y que es open source. Y precisamente con base en un audio, por ejemplo este. Este es el micromachinero presentando el modo de microchip más miniatura. Y como lo utilizamos dentro de un ecosistema en Python. Primero te tengo que decir que para crear ese ambiente en Python voy a utilizar Conda, que es básicamente el manejador de ambientes más recomendado para este tipo de ejemplos y de por sí es el más utilizado en toda la comunidad, sobre todo cuando ya tratamos de hacer cosas de inteligencia artificial. Normalmente sí, por ejemplo, estamos creando una aplicación web, normalmente utilizamos cosas como Docker o otras herramientas, pero en este caso Conda va a ser bastante interesante y voy a utilizarlo para crear un sistema de inteligencia artificial. Y para manejar este ambiente en el cual vamos a tener el proyecto en Python. Así que el primer paso será ir a tu terminal, crear una carpeta, en este caso le voy a poner Whisper Project. Luego pues voy a entrar a esa carpeta y dentro de esa carpeta voy a crear un ambiente. Yo ya tengo Conda instalado, pero realmente la instalación no es muy complicada, simplemente ve a la documentación y sigue el proceso. En la página web vas a encontrar el proceso para hacer la instalación en Windows, Mac o Linux. Entonces, vamos a crear un sistema de inteligencia artificial. Entonces, sigues este proceso y luego ya podrás seguir con tu instalación o la creación del ambiente en Conda. Si ya tienes la instalación, simplemente entonces le vamos a poner un nombre a ese ambiente, voy a llamarlo Whisper y luego voy a elegir en qué versión en específico quiero que este ambiente corra. En este caso quiero que corra bajo un ambiente en la versión de Python 3.10. Entonces voy a crear ese ambiente. Aquí básicamente me voy a instalar un ambiente como con algunas herramientas base que incluyen Python 3.10. Python y algunas otras herramientas. Así que simplemente le digo que sí para crear ese ambiente y ya. Una vez creado el ambiente tienes que activarlo. Y para activarlo simplemente de por si aquí te deja el comando, pero pues aquí lo vamos a activar. Es decir, es como entrar a ese ambiente. Y recuerda el nombre que le pusiste a ese ambiente. En ese caso, entonces se llama Whisper, el ambiente que creamos. Así que automáticamente ya entramos en el ambiente. Según la configuración. La configuración de terminal que tengas ya debería aparecer como el nombre del ambiente. Por ejemplo, en este caso aparece el nombre de mi ambiente que se llama Whisper. Listo, ya has creado un ambiente de Python con conda con la versión 3.10, pero ahora hay algunas herramientas de Python que necesitamos para que OpenAI, la librería de Whisper de OpenAI corra bastante bien. Lo primero es que lo que vamos a hacer es utilizar un gestor de dependencias. Yo sé que en Python normalmente utilizamos el manejador de paquetes de Python, que es pip, pero en este caso yo recomiendo Poetry, que es un manejador de ambientes que igual hace la gestión más fácil. Entonces vamos a instalar Poetry dentro de ese ambiente, pero también y algo mucho más importante es que tenemos que instalar una librería que tiene que estar en este ambiente para que Whisper funcione bien. Vamos a ver cuál es. Esta es la librería que necesita FFmpeg. Básicamente es una librería que es transversal y es cross-platform para manejar, convertir y procesar audio, video, etc. Esta librería la necesitamos dentro del ambiente porque Whisper lo va a utilizar, necesita de ella. Así que vamos a utilizar esta librería dentro del ambiente que necesitamos en Conda. Para hacer esa instalación, entonces como ya estamos dentro de nuestro ambiente de Whisper, vamos a decirle desde qué channel quiero instalar este paquete. Le voy a decir que desde Conda, Forge. Y luego le digo qué paquete queremos instalar. En teoría. Sería FFmpeg. Listo. Ah, es al revés. Listo. Ahí está. Y de una vez puedo decirle que también quiero instalar Poetry. Es el gestor de dependencias que te digo que vamos a tener en este proyecto. Lo instalamos y ya. Aquí siempre nos va a dar como un alerta de lo que va a instalar o todo lo que el sistema que está instalando. Porque una dependencia necesita de otras dependencias. Entonces le digo que sí, que todo bien. Y se va a instalar, pues, hacer toda la gestión. Listo. Una vez instalado. Entonces podemos hacer un Poetry. Como ya está instalado Poetry dentro de este ambiente. Va a reconocer el comando Poetry init. Y casi que le voy a decir a todo sí. Ahí está. Le digo que sí. Listo. Y ahí queda con un Poetry.init que es como la invocación o el manifest de las dependencias que tenemos en este ambiente. Una vez hecho esto, ahora sí con Poetry. Vamos a instalar o vamos a agregar una dependencia. Y en este momento. Y en este caso, pues, va a ser OpenAI Whisper. Que es la librería disponible en Python. Para poder, pues, agregar y manejar este modelo. Listo. Acá ya instaló OpenAI Whisper. Sin embargo, tengo que mencionarte que este proceso puede tardar. Depende mucho de tu conexión y también de tu computadora. Pero normalmente este proceso tarda un par de minutos. Así que ahorita con la magia de la edición vas a ver que esto fue súper rápido. Pero aún así, sí. Tú lo estás corriendo en tu máquina y quieres correr a este tipo. Pues, modelos normalmente van a tardar un rato. Un buen rato. Así que ten eso muy en cuenta. Ahora veamos, ahora sí, el proyecto y ver cómo configuramos y hacemos ya código. Si miramos de cerca al final el proyecto que tenemos aquí. O los archivos que tenemos en el proyecto. Es apenas, apenas lo que es los archivos de Poetry. Para poder gestionar y declarar las dependencias. Por ejemplo, aquí tenemos que tenemos Python 3.10. Y tenemos OpenAI Whisper. Esas son las dependencias que hemos gestionado con Poetry. Pero sabemos que nuestro entorno. Está con conda. Ya tiene FFmpeg. Tiene PyTorch. Tiene, pues como tal, Poetry instalado. Como parte de toda la dependencia. Ok. Entonces, precisamente ahora vamos a crear un archivo que se llama main.py. Y ese archivito es el que vamos a escribir. Pues todo nuestro código relacionado. Es más, aquí ya podemos hacer algo como import Whisper. Y va a salir sin ningún problema. Sin embargo, aquí. Recuerda. En Visual Studio Code. Debes igual verificar que estés ubicado en tu entorno. En el entorno que quedamos con conda. Y cómo puedes verificar esto. Fíjate que acá normalmente en la parte inferior te sale en qué ambiente estás ubicado. Por ejemplo, acá parece que es el ambiente de Whisper que está gestionado por conda. Básicamente, acá tienes puesta en varios ambientes. Acá literalmente está la versión de Python que tengas dentro de tu computadora. Puede tener. Es más, puedes a veces correr aquí una versión con Docker. Y decirle mira, esta es la versión que yo tengo. O en este caso estamos manejando conda y yo también tengo otros ambientes. Por ejemplo, qué pasa si lo meto o selecciono base. Entonces aquí, por ejemplo, Whisper como esa dependencia no está instalada dentro de mi entorno base en conda. Pues dice que hay un error. Entonces por eso tienes que checar muy bien que esté instalada. Si no te van a salir este tipo de errores como Whisper. ¿Cuál Whisper? Yo no tengo Whisper. O al menos no en el entorno. En el que lo estamos ejecutando. Entonces aquí yo le digo si mira Whisper el ambiente que está por conda. Ahí ya instalamos y ahí por ejemplo me dice listo. Si todo bien. Whisper si lo tengo dentro de ese ambiente. Es un error muy común no seleccionar el ambiente adecuado. Así que fíjate que tengas el ambiente que creaste con conda. Ok. Una vez hecho eso, entonces podemos crear ya la estructura y vamos a empezar a crear código. Entonces aquí, por ejemplo, vamos a definir una función. Pero antes de definir. La función precisamente vamos a cargar el modelo. Entonces aquí vamos a decirle Whisper punto y cargamos un modelo. Realmente hay varios modelos y ya depende es del nivel de precisión que tú tengas. También hay uno que es Small, pero depende mucho al a la precisión que tú necesites de la traducción o el transcript. Pues debe seleccionar el modelo adecuado. Si tienes duda, por ejemplo, aquí tienes la lista de modelos. Está el modelo Tiny. Base. Small. Medium. Y Latch. Fíjate que entre más va a requerir más cómputo más virtual RAM. Pero por ejemplo, el Latch puede ser como la base de todos y es como rápido. Sin embargo, si ya empezamos a bajar a Medium requiere menos RAM, pero se empiezan a demorar 2X, 6X. Por ejemplo, Tiny se demora 32X, entonces se demoraría un montón, pero gasta mucha menos RAM. Entonces ahí es donde tienes que escoger cuál sería el modelo más apropiado para ti. Según tu ambiente. También fíjate que hay otros modelos que si de pronto quieres solo enfocarte en audios que sean en inglés. Ya en inglés y ya puedes tener los modelos que solo son enfocados en el idioma inglés. Esos son los que son Tiny. En Base. En etcétera. Por ejemplo, parece que para Larch no hay un modelo exclusivo para el inglés. Sin embargo, estos son multi lenguaje. Qué quiere decir que podemos manejar audios en español. En alemán. En otros idiomas y él va a sacar el transcript, lo va a poder interpretar y pues va a poder sacar el transcript e inclusive traducir. Pero si solo queremos trabajar con modelos en inglés, pues también tenemos estos modelos ya exclusivos para esa tarea. Una vez entonces ya elegimos el modelo, vamos a crear una función que le vamos a llamar GetTranscript. Entonces vamos a poner acá, obtener el transcript y básicamente esto va a recibir dos parámetros. Un audio que sería como el pad o la ruta en la máquina en donde esté el audio como tal y el lenguaje y lo va a poner de por sí por defecto. Es decir, el lenguaje va a estar en en o sea como en inglés y listo y vamos a hacer. Ya aquí me auto completo el código. Entonces aquí literalmente como ya tenemos el modelo, entonces hacemos un punto transcript, mandamos el audio, mandamos el lenguaje. También le voy a agregar. Algo aquí que es el verbose que lo va a dejar en true, que es básicamente para ir de buggeando en real time. Que cuál es el proceso que él va a hacer y luego simplemente tenemos nuestra nuestro if name que es el que se encarga de hacer la ejecución. Ahí está y vamos a hacer la ejecución de nuestro método. Entonces, cómo sería? Vamos a igual a guardar esto en un resultado. Vamos a ver alguien transcript y lo primero que le tenemos que pasar es el audio. Bien. Entonces el audio lo vamos a poner aquí en un especial lado. Ahorita vamos a crear esa carpeta input es la audio punto. Ese va a ser el archivo. Realmente tú también le puedes pasar varios tipos de formato como punto MP3 o un web mobile, un web web M, que es básicamente un formato también de audio, pero para la web. En fin, el más confiable o el que tiene más precisión es un punto web, pero aún así el modelo es tan bueno que también puede tener otro tipo de. Formatos como inclusive MP3 y listo el lenguaje. Como ya sabemos que viene por defecto en inglés, pues simplemente se lo agregamos porque ya viene el barámetro por defecto. Ahora algo interesante y es que el lenguaje nosotros se lo estamos diciendo porque estamos teniendo el modelo que es el multimodal en donde puede seleccionar varios idiomas. Sin embargo, si tú le dices en qué lenguaje viene el audio, pues funciona bastante bien y funciona para que él no tenga que hacer la tarea de detectar. En qué lenguaje está, sino directamente le dices a mira este audio está en inglés a este audio está en alemán. Si es que supieras el idioma en el que está de pronto tú estás en un caso donde también necesitas como parte del modelo auto detectar el lenguaje y dejar que el modelo lo detecte y ya. Pero si supieras en qué lenguaje está el audio, pues simplemente le dices y le pasas el lenguaje y con eso pues ya el modelo se ahorraría un poco como el trabajo de hacer la autodetección. Listo finalmente, entonces aquí voy a. Imprimir. Como necesito aquí como una linecita para dividir en lo que ahorita me va a salir del verbose como del de book y ya del resultado y imprimamos entonces el resultado con un print. El resultado normalmente es un diccionario, entonces voy a tener una propiedad en diccionario que se llama text el texto y si no la tiene, pues igual voy a tener un vacío. Ok, ahora ya podríamos. Soltar esto. Escrita, pero nos falta el audio, el audio de no donde lo vamos a tener pues realmente pues o hacerlo desde. Cualquier modo puedes grabarte tú en un idioma en tu idioma y pues probarlo, entonces aquí crearíamos la carpeta input. Y vamos a crear ese audio o si quieres te voy a dejar un audio o unos tipos de audios en lo cual puedes probar con esos audios, pero realmente aquí ya puedes jalar donde quieras de tus recursos, de. Quieras jalar el. audio como tal, por ejemplo yo voy a utilizar estos audios, estos WAV de Enrollment Audio Kit, entonces este es un .wav, también aquí hay otro con Steve, .wav, normalmente estos son archivos de ejemplo que utiliza Microsoft para probar su servicio también de traducción, entonces también nos va a servir para probar Whisper, entonces aquí lo ponemos yo les voy a dejar el link aquí en la descripción del video pero simplemente descárgalo, ahí ya lo tenemos y lo vamos a llevar hacia nuestro ambiente, entonces aquí en mi carpeta de descargas está el archivo, lo copio y lo llevo al ambiente de Python entonces por acá lo copio y lo pego, ahí está, lo voy a pegar y es más, lo voy a cambiar de nombre para que sea audio.wav y es más, podemos reproducirlo, reproduzcámoslo un rato este es el audio básicamente es una persona hablando y lo que queremos hacer es sacar el transcript con el modelo de Whisper listo, entonces ahora llegó el momento de ejecutarlo, entonces vamos a nuestra terminal y ejecutámoslo realmente puedes utilizar la terminal en Python pero pues yo lo voy a hacer aquí desde mi terminal es más, si quieres ver que estructura hay, yo tengo este comandito que me deja ver la estructura de archivos entonces aquí por ejemplo al final nuestra estructura apenas es una carpeta input, un archivo y los archivos de Poetry para gestionar las dependencias ok, entonces vamos a ejecutar Python main.py y ver cuál es el resultado aquí por ejemplo tuve un error, vamos a corregirlo y ese error tiene que ver en que como ya utilicé esta forma de pasarle los atributos, también tengo que verificar que le llegue en esta forma, audio y language o sea, ser bastante explícito en la forma en que le estoy enviando cada uno, porque por ejemplo, pues verbose también se puede usar y language también está por acá entonces al final no sabe que atributo le estoy enviando, entonces le estoy diciendo, mira, audio es este, language es este y pues el verbose entró, ok, volvámoslo a probar, de nuevo entonces ejecutémoslo veamos, y listo, ahora sí tenemos nuestro audio acá vemos el debug como que está haciendo y por cada partecita dice en qué momento dice cierta frase o básicamente encierra, mira del minuto 00 al segundo 04, esto es lo que dice luego en el 04, el 07 dice esta otra parte, esto es el verbose 2 y sin embargo, pues como obtuvimos todo el texto, pues aquí literalmente ya me dices mira, el audio lo que dice es esto, entonces ya podemos comprobarlo con el audio como tal y en este caso ya tendríamos la transcripción de este audio basado o corriendo el modelo de whisper de OpenAI, aquí básicamente vamos a hacer una parte más y es exportar estas transcripciones en un formato adecuado, por ejemplo aquí ya tenemos el texto y lo tenemos ahí incluido en nuestro texto pero también podemos decirle al modelo que lo necesitamos en unos formatos que normalmente se utilizan en la industria para hacer subtitulación o para exportarlo casi como un ccb y poder, no sé, ponerlo o tabularlo en una base de datos etcétera, entonces vamos a utilizar, para hacer esa función de whisper, vamos a utilizar unas utils acá están las utils y vamos a importar el getWriter listo, de ahí entonces vamos a también crear un archivo o una función que se llame saveFiles, como guardar archivos listo, y vamos a ver que tiene este, pues este método entonces vamos a poner los resultados que tenemos del transcript y luego le vamos a decir el formato formato acá de por si mi github compiler me está como automáticamente completando código pero realmente está no lo voy a utilizar, pero si quiero un formato y el formato por defecto quiero que sea tsb, que es una especie de csb pero en vez de por comas es por tabulación, entonces realmente también podrías leer esto en un archivo ya de excel en un archivo que soporte este tipo de formato y ver que vas a obtener los segundos y las frases precisamente haciendo match entonces, pero también existen otro tipo de formatos, ahorita vamos a verlo entonces aquí vamos a poner el writer y le mandamos el formato ok, una vez eso hecho entonces vamos a hacer del writer vamos a escribir, vamos a mandarle los resultados aquí quedó un typo, results entonces ponemos results y finalmente entonces le vamos a decir el output en donde queremos que deje ese pues al final ese archivo, por ejemplo acá nos dice que en output.tsb lo voy a hacer como transcript acá está el archivo, transcript y listo, entonces ahí esa sería nuestra función, recuerda que recibe un formato y realmente ese formato son varios, entonces aquí voy a crear otra otra parte ejecutar, básicamente cuando ejecutemos nuestro script en la terminal, voy a decirle que aparte de imprimirme en texto pues vamos a mandarle resultados, aquí como el formato por defecto se le va a enviar, entonces aquí vamos a en tsb entonces pues él va a decir que va a utilizar este formato y listo es más creo que acá tengo un pequeño error si aquí tengo un error aquí necesita la carpeta es decir primero decirle output en qué carpeta lo va a dejar en qué carpeta va a trabajar y luego el formato en el cual lo va a dejar ese sería aquí antes de cometerlo y también pues ya que este formato va a ser como medio dinámico entonces voy a decirle que la extensión de ese formato aquí de este formato no acá pasamos el formato y la extensión la ponemos como una variable más forma entonces si éste se ve entonces esta va a ser la carpeta en la cual va a dejar los archivos y los bajar con este nombre y la extensión entonces vamos a crear también ese folder en este caso es output output auto y ahí esa es la carpeta en la que él va a trabajar y luego pues dejar el archivo entonces aquí cuando le digamos el file y no le enviamos el formato por defecto va a ser tsb pero podemos enviarle otro tipo entonces por ejemplo yo quiero un formato en texto texto plano que es va a ser casi igual al que tenemos acá que es el crudo básicamente el texto pero también podemos tener uno de los formatos más estándares para por ejemplo agregar subtitulación a vídeos que es este stsr2 vamos a ejecutar y ver si funciona aquí yendo a la terminal ejecutemos y veamos cómo funcionan estos formatos listo por acá dice que todo bien sin embargo falló algo aquí y es que la extensión no exceso este existe entonces vamos a corregir eso tienes que tener muy en cuenta cuáles son los formatos porque pues la extensión text no existe pero la extensión text es y si quieres mirar cuáles son los formatos que se pueden enviar puedes entrar acá el código de gato y estos son los formatos txt btt srt también en jason el formato que es como un csv pero con tabulaciones etcétera estos son los formatos no hay más estos son los formatos que le puedes enviar y estábamos cometiendo el error de pues básicamente enviarle mal uno que no es pues que desconoce entonces por eso nos lanzó error ahora ya corregir eso entonces vamos a volver a ejecutar y ver cuál es el resultado listo aquí ya no me sacó ningún error entonces si vamos de nuevo vamos a ver qué es lo que está pasando aquí en el formato txt y aquí está el formato srt que es básicamente te va diciendo mira esta es una parte es la segunda parte de este punto a este punto acá está esta palabra y también está el formato tsb que básicamente es también como un archivo csv pero por tabulaciones entonces esto ya lo puedes ver en excel o puedes leerlo en pandas por ejemplo o insertarlo ya una base de datos de transcripts y puedes ir mira ese es el transcript de este audio está dividido con precisión de esta sección a esta sección de esta sección hasta otra sección o por ejemplo simplemente tener el texto en crudo y ya que esto por ejemplo sirve mucho para inyectar reconocimiento a un large language model y uno le dice ok que dice este audio ya tienes el texto entonces lo puedes inyectar ya en crudo y empezar a hacer preguntas al chat como con modelos como gpt3 o gpt4 y decirle mira esto es lo que dice este audio ya tengo el texto y responde me preguntas acerca del audio no que tiene este texto ya que dice el audio envíame un summary de un resumen de lo que se dijo en el audio que básicamente así funcionan varias de las aplicaciones que ahorita ofrecen como el servicio de hacer transcript de de meetings de reuniones simplemente ponen el audio lo transcriben con un modelo como whisper y luego uno le puede hacer preguntas pasándole este crudo este texto y le dice ok por favor dime en cinco puntos que fue lo que se los puntos claves de la reunión por ejemplo no y le pasas este contexto el crudo del audio y él va a empezar a hacer resumen Ahora si eso es un transcript muy muy largo por ejemplo este es un transcript corto pero si es uno muy muy largo realmente por ejemplo ahorita los modelos como gpt4 o gpt3 turbo tienen una gran ventana de contexto entonces puedes inyectar de todo todo no sé esa hora de audio podría ser pero si ya te pasas de este contexto y es mucho pues básicamente tocas ver un poco ya de y de pronto utilizar una base de datos vectorial, etc. Pero, en fin, eso será para otro video. Pero por ahora, fíjate que ya tienes el transcript. Ya, ahora me gustaría hacer una cosa, comparémoslo realmente. A ver, vamos a dividir pantalla. Acá tenemos el audio y acá tenemos como lo que dice. Y veamos si se hace match como tal. Listo, aquí vemos literalmente que ese audio que fue en inglés, realmente la transcripción es muy, muy buena. Y también tenemos la precisión de en qué momento eso ocurre. Es eso que ya está hablando. Si quieres los pasos de forma resumida, recuerda que te voy a dejar este link dentro de la descripción del video en donde le pongo el link. Literalmente hacemos los pasos de la instalación del ambiente, lo que hicimos con Whisper, escoger el modelo. Literalmente está el código que ya ejecutamos, el audio que utilicé. Entonces lo puedes aquí descargar directamente, ejecutarlo. Y también tienes la parte de los archivos. Esto fue al final a lo que llegamos. Y pues cómo quedaron las transcripciones. Y al final, pues cómo quedó ya la estructura de nuestro proyecto. Y lo hemos logrado. Ya precisamente tienes el modelo de Whisper corriendo de forma local. En tu máquina. Ahora, realmente tengo que hacer algunas aclaraciones. Y estos modelos normalmente consumen bastante RAM. Sobre todo si escoges el modelo más pesado. Pero también requieren en su mayoría de casos de tarjeta gráfica. Entonces si tú tienes un computador que no tiene una tarjeta gráfica, pues va a ser un poco complicado correr este tipo de modelos. Como ves, también se puede correr en CPU. Si eliges PyTorch y eliges la instalación de CPU, lo corre con el procesador de tu máquina. Va a funcionar normalmente. Se va a demorar. Y si tienes un modelo más pequeño, si tienes CPU, de pronto la transcripción no va a tener el mismo nivel de calidad. Pero lo vas a poder probar. Pruébalo con Tiny o algún modelo Small. Si tienes tarjeta gráfica, genial. Lo puedes hacer con los modelos más fuertes y que requieren mucho más de RAM. Y aquí precisamente entonces hay una discusión muy interesante que te quiero dejar porque lo voy a ir abordando en otros videos. Y es, ok, ya puedes tener modelos, que corran en tu máquina. Pero normalmente, si tú ya quieres crear aplicaciones empresariales, aplicaciones que utilicen este tipo de modelos como features, como por ejemplo una aplicación de traducción, una aplicación que tú le mandes el audio, lo transcriba y luego pues hagas un GPT o una versión de GPT que responda a esos transcripts y muchas ideas que pueden surgir de ahí. Pues aquí está el problema. Pues precisamente, tú por ejemplo, este código en Python ya lo podrías poner en una API, con FastAPI. Entonces, tú puedes crear tu API para recibir audios, guardarlos en algún storage, S3 de Amazon, por ejemplo, y empezar a hacer la transcripción. Puedes utilizar una cola de tareas con Celery también en Python para procesar, por ejemplo, las tareas de acuerdo a que te vayan llegando solicitudes de transcripción, irlas operando. Pero el problema radica también entonces que en donde corran, en el servidor donde corran este tipo de modelos, pues tiene que ser una máquina en donde tenga acceso a buena GPU. Normalmente, ahorita también se está volviendo una tendencia en que proveedores como Amazon, como Google Cloud, como DigitalOcean, tienen servidores especializados, no sólo con buena RAM, sino CPU, sino con una alta GPU. Por ejemplo, veamos el caso de DigitalOcean. Cada nube es diferente, pero por ejemplo, la de DigitalOcean, acá tenemos droplets que son como servidores o instancias, normales, como es probar Kubernetes, pero ellos tienen, por ejemplo, un servicio que se llama Paperspace, en donde son máquinas que tienen bastante GPU para poder correr modelos. Entonces, precisamente, si tú vas a desarrollar una API que utiliza un modelo, pues ya no correría en tu máquina, en tu computadora con una buena GPU o no, con CPU, pero si ya quieres llevarlo a la nube, pues esa máquina también tendría que tener acceso a una buena GPU para que corra rápido. Sin embargo, hay una tendencia, muy interesante, y es que modelos de Cloud ya están poniendo estos modelos open source con su gran infraestructura dentro de motores de inferencia, dentro de las nubes. ¿Eso qué quiere decir? Que tú no te tienes que preocupar por gestionar o mantener una máquina que esté bien adecuada, que tenga una buena GPU, sino simplemente a partir de una API o a través de un servicio, se consumen este tipo de modelos. Por ejemplo, Microsoft con el Azure AI Studio está disponiendo un catálogo de modelos que tú puedes simplemente decir, quiero utilizar este modelo, y ellos ya se encargan de hacerla, como ejecutarlo en sus servidores con la GPU necesaria. Para ti, simplemente es una API plana. Y ya, mandas tu audio y obtienes el transcript. Obviamente, esto ya va a tener un costo y depende mucho de la nube, pero básicamente estos costos se están abaratando. Y, por ejemplo, aquí en el modelo del Azure AI Studio, tienes un catálogo inmenso de modelos, que no te tienes que preocupar como realmente ver si lo tienes que correr o no. Simplemente aquí, por ejemplo, ponemos Whisper. Vamos a ver si está. Aquí, por ejemplo, dice que está Whisper en su, con el modelo Large. Y luego nos da la documentación de cómo podríamos, entonces, empezar a, pues, utilizarlo ya como un servicio. Acá, por ejemplo, nos dice el modelo, cómo lo va a utilizar, y nos puede empezar a dar un poquito de código. Es más, aquí nos da una notebook para decir cómo construirlo. ¿Cómo consumiríamos ese modelo dentro de la nube de Azure? O, por ejemplo, también tenemos otras cosas como Cloudflare, que también tiene ya modelos disponibles de inteligencia artificial. Entre esos también tiene Whisper. Entonces, acá, por ejemplo, vamos a ver si acá está. Acá está Speech Recognition. Y tú puedes utilizar, crearte tu cuenta en Cloudflare y no tener que mantener el modelo, sino simplemente utilizarlo como una API más, o lo más fácil, literalmente, OpenAI dentro de su librería, tiene, pues, tiene un SDK. Acá lo importas. Esta aquí ya no necesitarías, por ejemplo, de usar PyTorch o de, pues, utilizar tantas dependencias o tanta máquina, porque al final, esta librería de OpenAI para Python, pues, al final llama una API, una API REST, en donde le envías el audio y obtienes el transcript. Pero aquí, obviamente, tiene un costo. Entonces, vas a tu plataforma, te van a decir, ¿cuánto es el costo de empezar a hacer esas transcripciones? Envías tu audio y simplemente lo recibes. Pero aquí ya no estás ejecutando el modelo local, sino lo está ejecutando la nube de OpenAI o de Cloudflare o de Azure, etcétera, la que tú ya elijas. Eso ha sido todo por el video de hoy. Precisamente, dale like, suscríbete al canal. Voy a empezar a subir como contenido así relacionado con inteligencia artificial. Primero, me gusta mucho prototipar este tipo de modelos con Python, porque normalmente es donde se tiene más acceso rápido, o hay mucha más documentación de cómo correr o ejecutar este tipo de modelos. Pero luego, la gracia es precisamente crear aplicaciones. Por ejemplo, me gusta mucho Angular desde el lado del frontend. Entonces, crear como realmente aplicaciones ya productivas que utilizan Angular y una API, por ejemplo, utilizando el motor de inferencia de Cloudflare, que es casi gratis, tiene costos muy, muy bajos, en donde pues utilicemos Whisper, que ya está ahí alojado en Cloudflare y simplemente lo ejecutamos, creamos una API y casi que tendría, podría tener un costo cero y es una aplicación que ya está casi lista para empezar a utilizar en producción. Así que dale like si quieres que continúe con este serie de videos de hacer aplicaciones basadas en inteligencia artificial. Nos vemos. Hasta la próxima.